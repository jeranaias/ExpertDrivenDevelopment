<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Course 4: Advanced Workshop - A 4-hour elective covering frontier mapping, multi-component builds, verification protocols, group debugging, and teaching others.">
    <title>Advanced Workshop | Expert-Driven Development</title>
    <link rel="stylesheet" href="../css/style.css">
</head>
<body class="site-wrapper">

    <!-- Skip to content -->
    <a href="#main-content" class="skip-link">Skip to main content</a>

    <!-- Site Header -->
    <header class="site-header" role="banner">
        <div class="container">
            <a href="../" class="site-brand">
                <span class="site-brand__mark" aria-hidden="true"></span>
                Expert-Driven Development
            </a>
            <nav aria-label="Primary navigation">
                <ul class="nav-list">
                    <li><a href="../">Home</a></li>
                    <li><a href="./" class="active">Courses</a></li>
                    <li><a href="../sop/">SOP</a></li>
                    <li><a href="../toolkit/">Toolkit</a></li>
                    <li><a href="../resources/">Resources</a></li>
                    <li><a href="../about.html">About</a></li>
                </ul>
            </nav>
            <button class="nav-toggle" aria-label="Toggle navigation menu" aria-expanded="false" aria-controls="mobile-nav">
                <span class="nav-toggle__icon" aria-hidden="true"></span>
            </button>
        </div>
    </header>

    <!-- Mobile Navigation -->
    <div id="mobile-nav" class="mobile-nav" role="navigation" aria-label="Mobile navigation">
        <ul class="nav-list">
            <li><a href="../">Home</a></li>
            <li><a href="./" class="active">Courses</a></li>
            <li><a href="../sop/">SOP</a></li>
            <li><a href="../toolkit/">Toolkit</a></li>
            <li><a href="../resources/">Resources</a></li>
            <li><a href="../about.html">About</a></li>
        </ul>
    </div>

    <!-- Page Header -->
    <section class="page-header">
        <div class="container">
            <nav aria-label="Breadcrumb">
                <ol class="breadcrumb">
                    <li><a href="../">Home</a></li>
                    <li><a href="./">Courses</a></li>
                    <li aria-current="page">Advanced Workshop</li>
                </ol>
            </nav>
            <h1 class="page-header__title">Course 4: Advanced Workshop</h1>
            <p class="page-header__desc">
                4 hours &mdash; Move from individual capability to organizational capability.
                Map the frontier for your domain, build verification protocols, and learn to teach others.
            </p>
            <div class="mt-lg">
                <span class="badge">Elective</span>
                <span class="badge">4 Hours</span>
                <span class="badge badge--gold">Experienced Builders</span>
            </div>
            <p class="mt-lg"><strong>Prerequisite:</strong> At least one deployed tool</p>
            <p><a href="student/advanced.html">View the Student Version</a></p>
        </div>
    </section>

    <!-- Main Content -->
    <main id="main-content" class="site-main">
        <div class="container">
            <div class="page-layout page-layout--sidebar">

                <!-- Sidebar / Table of Contents -->
                <aside class="sidebar sidebar--sticky" aria-label="Table of contents">
                    <nav class="toc">
                        <h2 class="toc__title">On This Page</h2>
                        <ul class="toc__list">
                            <li><a href="#instructor-qualification">Instructor Qualification</a></li>
                            <li><a href="#timing">Timing Breakdown</a></li>
                            <li><a href="#agenda">Agenda</a></li>
                            <li><a href="#frontier-mapping">Frontier Mapping for Your Domain</a></li>
                            <li><a href="#complex-build">Complex Build: Multi-Component System</a></li>
                            <li><a href="#verification">Verification Protocols and QA</a></li>
                            <li><a href="#group-debugging">Group Debugging: Real Problems</a></li>
                            <li><a href="#teaching-others">Teaching Others: The 201 Multiplier</a></li>
                            <li><a href="#assessment">Assessment Rubric</a></li>
                        </ul>
                    </nav>
                </aside>

                <!-- Page Content -->
                <div class="page-content">

                    <!-- Instructor Qualification -->
                    <section id="instructor-qualification">
                        <div class="callout callout--warning">
                            <h3 class="callout__title">Instructor Qualification Note</h3>
                            <p>
                                This course requires an instructor who has personally built and deployed at least 3 tools using AI assistance. The complex build in Module 2 requires real-time troubleshooting ability. You cannot effectively teach this course from a script alone. Students will encounter unexpected errors, frontier limitations, and integration failures. The instructor must be able to diagnose problems on the fly, guide students through iterative refinement, and recognize when a problem is a frontier issue versus a context/prompting issue.
                            </p>
                        </div>
                    </section>

                    <!-- Timing Breakdown -->
                    <section id="timing">
                        <h2>Timing Breakdown</h2>
                        <div class="table-wrapper">
                            <table>
                                <thead>
                                    <tr>
                                        <th scope="col">Module</th>
                                        <th scope="col">Duration</th>
                                        <th scope="col">Cumulative Time</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <tr>
                                        <td>Module 1: Frontier Mapping</td>
                                        <td>30 min</td>
                                        <td>0:30</td>
                                    </tr>
                                    <tr>
                                        <td>Module 2: Complex Build</td>
                                        <td>60 min</td>
                                        <td>1:30</td>
                                    </tr>
                                    <tr>
                                        <td>Break</td>
                                        <td>10 min</td>
                                        <td>1:40</td>
                                    </tr>
                                    <tr>
                                        <td>Module 3: Group Debugging</td>
                                        <td>40 min</td>
                                        <td>2:20</td>
                                    </tr>
                                    <tr>
                                        <td>Module 4: Verification &amp; QA</td>
                                        <td>30 min</td>
                                        <td>2:50</td>
                                    </tr>
                                    <tr>
                                        <td>Module 5: Teaching Methodology</td>
                                        <td>30 min</td>
                                        <td>3:20</td>
                                    </tr>
                                    <tr>
                                        <td>Module 6: Workflow Playbook &amp; Wrap-Up</td>
                                        <td>30 min</td>
                                        <td>3:50</td>
                                    </tr>
                                    <tr>
                                        <td>Buffer</td>
                                        <td>10 min</td>
                                        <td>4:00</td>
                                    </tr>
                                </tbody>
                            </table>
                        </div>
                        <p class="mt-md">
                            <strong>Total:</strong> 230 minutes (3 hours 50 minutes). The 10-minute buffer accounts for technical issues, extended Q&amp;A, or students who need additional troubleshooting time during the complex build.
                        </p>
                    </section>

                    <!-- Agenda -->
                    <section id="agenda">
                        <h2>Agenda</h2>
                        <div class="table-wrapper">
                            <table>
                                <thead>
                                    <tr>
                                        <th scope="col">Time</th>
                                        <th scope="col">Module</th>
                                        <th scope="col">201 Skills Applied</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <tr>
                                        <td>0:00&ndash;0:30</td>
                                        <td>Module 1: Frontier Mapping for Your Domain</td>
                                        <td>Frontier Recognition</td>
                                    </tr>
                                    <tr>
                                        <td>0:30&ndash;1:30</td>
                                        <td>Module 2: Complex Build (Unit Readiness Dashboard)</td>
                                        <td>All six skills (multi-step prompting, context management, mode-switching, iterative refinement, error recovery, verification)</td>
                                    </tr>
                                    <tr>
                                        <td>1:30&ndash;1:40</td>
                                        <td>Break</td>
                                        <td></td>
                                    </tr>
                                    <tr>
                                        <td>1:40&ndash;2:20</td>
                                        <td>Module 3: Group Debugging (Real Problems)</td>
                                        <td>Iterative Refinement, Frontier Recognition, Diagnostic Thinking</td>
                                    </tr>
                                    <tr>
                                        <td>2:20&ndash;2:50</td>
                                        <td>Module 4: Verification Protocols and QA</td>
                                        <td>Quality Judgment, Systematic Review</td>
                                    </tr>
                                    <tr>
                                        <td>2:50&ndash;3:20</td>
                                        <td>Module 5: Teaching Others (Teach-Back Exercise)</td>
                                        <td>Communication, Knowledge Transfer</td>
                                    </tr>
                                    <tr>
                                        <td>3:20&ndash;3:50</td>
                                        <td>Module 6: Workflow Playbook Creation</td>
                                        <td>Documentation, Process Design</td>
                                    </tr>
                                    <tr>
                                        <td>3:50&ndash;4:00</td>
                                        <td>Wrap-Up &amp; Next Steps</td>
                                        <td></td>
                                    </tr>
                                </tbody>
                            </table>
                        </div>
                        <p class="mt-md">
                            <strong>Total:</strong> 4 hours (includes 10-minute break and 10-minute buffer built into timing)
                        </p>
                    </section>

                    <!-- Module 1: Frontier Mapping for Your Domain -->
                    <section id="frontier-mapping">
                        <h2>Module 1: Frontier Mapping for Your Domain</h2>
                        <p><strong>Duration:</strong> 30 minutes</p>

                        <p>
                            Each participant creates a frontier map for their domain.
                        </p>

                        <h3>Frontier Map Format</h3>
                        <div class="table-wrapper">
                            <table>
                                <thead>
                                    <tr>
                                        <th scope="col">Category</th>
                                        <th scope="col">Inside Frontier (AI Handles)</th>
                                        <th scope="col">Outside Frontier (AI Fails)</th>
                                        <th scope="col">Moving Frontier (Check Periodically)</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <tr>
                                        <td>Document generation</td>
                                        <td>Correspondence drafts, counseling statement templates, award write-ups, standard memo formatting</td>
                                        <td>Documents requiring specific institutional knowledge (unit SOPs, local policy interpretation), anything needing exact regulation quotes</td>
                                        <td>Fitness report narratives (improving rapidly), legal review summaries</td>
                                    </tr>
                                    <tr>
                                        <td>Data analysis</td>
                                        <td>Trend identification in structured data, summarizing large datasets, creating charts/dashboards, anomaly flagging</td>
                                        <td>Interpreting data in operational context (why readiness dropped), cross-referencing classified and unclassified sources</td>
                                        <td>Predictive analysis (retention modeling, maintenance forecasting)</td>
                                    </tr>
                                    <tr>
                                        <td>Process automation</td>
                                        <td>Simple approval routing, email notifications, calendar scheduling, status tracking</td>
                                        <td>Multi-system integrations with legacy databases, processes requiring human judgment calls (hardship determinations)</td>
                                        <td>Complex conditional workflows (getting better with clear business rules)</td>
                                    </tr>
                                    <tr>
                                        <td>Reference lookup</td>
                                        <td>Finding relevant MCOs/NAVMCs, summarizing policy documents, comparing regulation versions</td>
                                        <td>Interpreting how regulations apply to specific edge cases, resolving conflicting guidance between orders</td>
                                        <td>Policy applicability questions (models improving but still unreliable for authoritative interpretation)</td>
                                    </tr>
                                    <tr>
                                        <td>Training development</td>
                                        <td>Lesson plan outlines, quiz/assessment generation, scenario creation, slide deck structure</td>
                                        <td>Evaluating training effectiveness, adapting content for specific MOS requirements, determining doctrinal accuracy</td>
                                        <td>Full lesson plan generation with appropriate examples (quality varies significantly)</td>
                                    </tr>
                                </tbody>
                            </table>
                        </div>

                        <p>
                            <strong>Deliverable:</strong> One-page frontier map published on the EDD site.
                        </p>

                        <div class="callout callout--info">
                            <h3 class="callout__title">Key Teaching Point</h3>
                            <p>
                                This map is the most valuable artifact. It prevents the
                                19-percentage-point performance drop from the BCG-Harvard study.
                                When workers apply AI beyond the frontier without knowing it,
                                quality collapses. The frontier map makes the boundary visible.
                            </p>
                        </div>
                    </section>

                    <!-- Module 2: Complex Build — Multi-Component System -->
                    <section id="complex-build">
                        <h2>Module 2: Complex Build &mdash; Multi-Component System</h2>
                        <p><strong>Duration:</strong> 60 minutes</p>

                        <p>
                            This is the most complex build in the entire curriculum. Students will build a Unit Readiness Dashboard that pulls from multiple data sources, requires 8-10 sequential prompts, and demonstrates advanced techniques including multi-step prompting, context management, and error recovery. This build should feel significantly harder than anything in Platform Training.
                        </p>

                        <div class="callout callout--instructor">
                            <h3 class="callout__title">Instructor Note: Mode-Switching is the Goal</h3>
                            <p>
                                The goal is conscious, deliberate mode-switching, not a polished product. Assess students on their decision-making process, not on output quality. Watch for students who understand when to slow down for accuracy-critical work versus when to iterate rapidly. This is the key skill.
                            </p>
                        </div>

                        <h3>Build Goal: Unit Readiness Dashboard for 1st Bn, 99th Marines</h3>
                        <p>
                            Create a Power BI dashboard that consolidates:
                        </p>
                        <ul>
                            <li>Training status (pulled from Excel training tracker)</li>
                            <li>Equipment status (pulled from GCSS-MC export or simulated CSV)</li>
                            <li>Personnel status (pulled from Alpha roster or simulated data)</li>
                        </ul>
                        <p>
                            The dashboard should display overall readiness percentage, breakdowns by company, and flag critical shortfalls.
                        </p>

                        <h3>Phase 1: Data Architecture (Centaur Mode &mdash; 15 minutes)</h3>
                        <p>
                            <strong>Why Centaur:</strong> Data schema errors compound throughout the project. A bad data model means rework. Slow down and verify.
                        </p>

                        <div class="callout callout--instructor">
                            <h3 class="callout__title">Instructor Checkpoint</h3>
                            <p>
                                Before students begin, ask: "Why are we using centaur mode for this phase?" Correct answer: "Because the data model is the foundation. If we get this wrong, everything built on top of it breaks. We need to verify the schema before moving forward."
                            </p>
                        </div>

                        <h4>Step-by-Step Prompting Sequence</h4>

                        <div class="prompt-block">
                            <strong>Prompt 1:</strong> I need to build a readiness dashboard for a Marine Corps infantry battalion. I have three data sources: a training tracker (Excel), an equipment status report (CSV export from GCSS-MC), and a personnel roster (Alpha roster). What data model should I use to consolidate these sources in Power BI? Provide a star schema with fact and dimension tables.
                        </div>

                        <div class="code-block">
                            <strong>Expected Output:</strong> AI should propose a star schema with a central fact table (Readiness_Fact) and dimension tables for Personnel, Training, Equipment, and Time. The schema should include foreign keys linking the fact table to dimensions.
                        </div>

                        <div class="callout callout--instructor">
                            <h3 class="callout__title">Instructor Note: Verification Point</h3>
                            <p>
                                Stop here. Have students manually verify the proposed schema against their actual data sources. Common AI errors: proposing columns that don't exist in the source data, misunderstanding military data structures (e.g., confusing EDIPI with DOD ID), or creating overly complex schemas that won't work with the actual data.
                            </p>
                        </div>

                        <div class="prompt-block">
                            <strong>Prompt 2:</strong> Here is the structure of my actual training tracker: [paste first 5 rows with headers]. Does your proposed schema still work, or do we need to adjust the column mappings?
                        </div>

                        <div class="code-block">
                            <strong>Expected Output:</strong> AI will identify mismatches and propose adjustments. Common issue: AI assumes standardized column names, but real data uses abbreviations or non-standard formats.
                        </div>

                        <div class="callout callout--instructor">
                            <h3 class="callout__title">Decision Point: Ask Students</h3>
                            <p>
                                At this point, ask: "The AI proposed renaming columns in your source data. Should you rename the columns in the Excel file, or handle the mapping in Power Query?" Discuss tradeoffs: renaming source data breaks existing workflows; mapping in Power Query adds transformation complexity but preserves source integrity.
                            </p>
                        </div>

                        <h3>Phase 2: Data Ingestion (Cyborg Mode &mdash; 15 minutes)</h3>
                        <p>
                            <strong>Why Cyborg:</strong> Data ingestion involves iteration and trial-and-error. Connection strings fail, file paths break, data types mismatch. Rapid iteration is more valuable than slow verification.
                        </p>

                        <div class="prompt-block">
                            <strong>Prompt 3:</strong> Write the Power Query M code to import my training tracker Excel file from [file path], select the relevant columns [list columns], and transform the "Training Date" column from text to date format.
                        </div>

                        <div class="code-block">
                            <strong>Expected Output:</strong> M code block with Source, Navigation, Column Selection, and Type Transformation steps.
                        </div>

                        <div class="callout callout--instructor">
                            <h3 class="callout__title">Common Error: This Will Likely Fail</h3>
                            <p>
                                Students will copy-paste the M code and it will fail. This is expected. Common causes: file path has spaces and isn't escaped, Excel sheet name is wrong, column names have special characters. This is a teaching moment: "This is cyborg mode. The first output failed. What's the error message? Feed it back to the AI."
                            </p>
                        </div>

                        <div class="prompt-block">
                            <strong>Prompt 4 (Error Recovery):</strong> I got this error: [paste error message]. Here's the actual structure of my Excel file: [paste screenshot or description]. Fix the M code.
                        </div>

                        <div class="code-block">
                            <strong>Expected Output:</strong> Corrected M code. AI will adjust for actual file structure.
                        </div>

                        <div class="callout callout--instructor">
                            <h3 class="callout__title">Teaching Point: Error Recovery</h3>
                            <p>
                                Highlight this moment. "Notice how we didn't start over. We gave the AI the error message and context. This is how you work in cyborg mode: fast iteration, immediate feedback, continuous refinement."
                            </p>
                        </div>

                        <div class="prompt-block">
                            <strong>Prompt 5:</strong> Now write the M code to import the equipment status CSV from [file path]. The CSV has columns: Equipment_ID, Nomenclature, Status_Code, Assigned_Unit. Convert Status_Code to a descriptive status (FMC, PMC, NMC).
                        </div>

                        <div class="prompt-block">
                            <strong>Prompt 6:</strong> Write the M code to import the personnel roster from [file path]. The roster has: Last_Name, First_Name, Rank, EDIPI, Unit, Billet. Create a full name column combining First and Last names.
                        </div>

                        <h3>Phase 3: Dashboard Visualization (Centaur Mode &mdash; 20 minutes)</h3>
                        <p>
                            <strong>Why Centaur:</strong> The dashboard is the end product. Leadership will make decisions based on this output. Accuracy is critical. Slow down and verify every number.
                        </p>

                        <div class="prompt-block">
                            <strong>Prompt 7:</strong> Create a DAX measure to calculate overall unit readiness percentage. Readiness is defined as: (Number of personnel with all required training complete AND assigned equipment FMC) / (Total personnel). Show the formula.
                        </div>

                        <div class="code-block">
                            <strong>Expected Output:</strong> DAX formula using CALCULATE, FILTER, and COUNTROWS functions.
                        </div>

                        <div class="callout callout--instructor">
                            <h3 class="callout__title">Verification Checkpoint</h3>
                            <p>
                                Stop here. Have students manually calculate readiness for one company using a calculator. Does the DAX formula produce the same number? If not, the formula is wrong. This is centaur mode: verify before proceeding.
                            </p>
                        </div>

                        <div class="prompt-block">
                            <strong>Prompt 8:</strong> Create a card visual showing overall readiness percentage. If readiness is below 75%, the card should display red. If between 75-90%, yellow. If above 90%, green.
                        </div>

                        <div class="prompt-block">
                            <strong>Prompt 9:</strong> Create a bar chart showing readiness percentage by company (Alpha, Bravo, Charlie, Weapons). Sort by readiness descending.
                        </div>

                        <div class="prompt-block">
                            <strong>Prompt 10:</strong> Create a table visual showing all personnel who are NOT fully ready (missing training or equipment NMC). Columns: Name, Rank, Unit, Missing_Training, Equipment_Status. Sort by unit, then by rank descending.
                        </div>

                        <div class="callout callout--instructor">
                            <h3 class="callout__title">Final Verification</h3>
                            <p>
                                Before students present their dashboards, run this verification protocol: "Pick one Marine from the 'Not Ready' table. Manually trace their data back through the source files. Are they actually missing training? Is their equipment actually NMC? If the dashboard says they're not ready, can you prove it from the source data?" This is the QA step. If students can't verify their output, the dashboard isn't ready.
                            </p>
                        </div>

                        <h3>Debrief Questions (10 minutes)</h3>
                        <ul>
                            <li>Where did you switch modes? Why?</li>
                            <li>Where did the AI fail? Was it a frontier issue or a context issue?</li>
                            <li>If you had to build this again, what would you do differently?</li>
                            <li>How much time did this take? How long would it have taken without AI?</li>
                        </ul>

                        <div class="callout callout--success">
                            <h3 class="callout__title">Key Takeaway</h3>
                            <p>
                                This build required 10 prompts, 3 mode switches, and at least 2 error recovery cycles. This is normal for complex builds. The students who succeeded were the ones who verified at each phase boundary, recognized when AI output was wrong, and knew how to feed errors back into the conversation for refinement.
                            </p>
                        </div>
                    </section>

                    <!-- Module 3: Verification Protocols and QA -->
                    <section id="verification">
                        <h2>Module 4: Verification Protocols and QA</h2>
                        <p><strong>Duration:</strong> 30 minutes</p>

                        <p>
                            Build a QA checklist for your domain. For AI-generated output, what must be checked?
                        </p>

                        <h3>QA Checklist</h3>
                        <ol>
                            <li>
                                <strong>Source verification</strong> &mdash; AI fabricates references.
                                Every citation, regulation number, and URL must be independently verified.
                            </li>
                            <li>
                                <strong>Data accuracy</strong> &mdash; Numbers, dates, names, and quantities
                                must be checked against source data.
                            </li>
                            <li>
                                <strong>Logic check</strong> &mdash; Does the reasoning hold? Are conclusions
                                supported by the premises?
                            </li>
                            <li>
                                <strong>Format compliance</strong> &mdash; Does the output match required formats,
                                templates, and standards?
                            </li>
                            <li>
                                <strong>Domain review</strong> &mdash; Does this pass the smell test for someone
                                who knows this domain?
                            </li>
                        </ol>

                        <h3>Exercise: Timed QA Review</h3>
                        <p>
                            Take an AI-generated document. Run it through your checklist. Time yourself.
                            Compare QA time versus creation-from-scratch time. Usually 30&ndash;50% &mdash;
                            that's the time savings.
                        </p>

                        <div class="template-display">
                            <h4>AI-Generated SOP Excerpt &mdash; QA Timed Exercise</h4>
                            <p><strong>Instructions:</strong> Time yourself. How long does it take you to find all five issues? Typical completion time: 5&ndash;10 minutes.</p>
                            <hr>
                            <p><strong>STANDARD OPERATING PROCEDURE</strong><br>
                            Marine Corps Detachment, 99th Training Group<br>
                            <strong>Subject:</strong> Unit Check-In / Check-Out Procedure<br>
                            <strong>Reference:</strong> (a) MCO 1000.6B, Individual Records Administration<br>
                            (b) NAVMC 11800/4 (Rev 03-2025), Check-In/Check-Out Sheet</p>

                            <p><strong>1. Purpose.</strong> To establish standardized procedures for all personnel
                            checking in to and checking out of Marine Corps Detachment, 99th Training Group.
                            All personnel shall complete check-in within 72 hours of reporting aboard.</p>

                            <p><strong>2. Scope.</strong> This SOP applies to all Marines, Sailors, and civilian
                            personnel assigned to or transferring from the Detachment.</p>

                            <p><strong>3. Procedure &mdash; Check-In.</strong> Personnel reporting aboard shall
                            complete the following steps in order:</p>

                            <p>Step 1: Report to the Officer of the Day (OOD) with original orders and ten
                            copies of PCS orders.<br>
                            Step 2: Obtain a check-in sheet per reference (b).<br>
                            Step 3: Receive unit orientation brief from S-1 covering unit organization,
                            key personnel, and local policies.<br>
                            Step 4: Report to assigned section SNCOIC/OIC for introduction and initial
                            task assignment.<br>
                            5. Report to S-1 for initial in-processing, including service record book
                            review and page 11 entry.<br>
                            6. Complete remaining check-in sheet signatures (S-3, S-4, Medical, Dental,
                            IPAC) within 48 hours of reporting.</p>

                            <p><strong>4. Procedure &mdash; Check-Out.</strong> Personnel transferring from
                            the unit shall initiate check-out procedures no later than 10 working days
                            prior to the date of detachment.</p>
                        </div>

                        <details>
                            <summary><strong>Answer Key &mdash; Five Planted Errors</strong></summary>
                            <ol>
                                <li>
                                    <strong>Fabricated Reference #1:</strong> &ldquo;MCO 1000.6B&rdquo; is cited as the governing order for check-in procedures. This MCO does not exist. AI-generated regulation numbers must always be independently verified against the official Marine Corps Publications System.
                                </li>
                                <li>
                                    <strong>Fabricated Reference #2:</strong> &ldquo;NAVMC 11800/4 (Rev 03-2025)&rdquo; is cited as the check-in/check-out form. This form number is fabricated. AI frequently generates plausible-sounding form numbers that do not correspond to real NAVMC forms.
                                </li>
                                <li>
                                    <strong>Data Accuracy Error &mdash; Contradictory Timelines:</strong> Paragraph 1 states check-in must be completed &ldquo;within 72 hours of reporting aboard,&rdquo; but Step 6 states remaining signatures must be completed &ldquo;within 48 hours of reporting.&rdquo; These timelines contradict each other. AI often introduces subtle inconsistencies between sections of longer documents.
                                </li>
                                <li>
                                    <strong>Logic Error &mdash; Steps Out of Order:</strong> Step 3 has the Marine receiving a &ldquo;unit orientation brief from S-1,&rdquo; but Step 5 has the Marine reporting &ldquo;to S-1 for initial in-processing.&rdquo; Logically, you would in-process at S-1 (Step 5) before receiving the orientation brief (Step 3). The S-1 steps are reversed.
                                </li>
                                <li>
                                    <strong>Format Error &mdash; Inconsistent Numbering:</strong> Steps 1 through 4 use the &ldquo;Step 1:&rdquo; format, but the procedure then switches to a bare &ldquo;5.&rdquo; and &ldquo;6.&rdquo; format midway through. AI frequently loses formatting consistency in longer documents, especially when generating numbered procedures.
                                </li>
                            </ol>
                        </details>

                        <div class="callout callout--info">
                            <h3 class="callout__title">Key Teaching Point</h3>
                            <p>
                                The GDPval study found human experts averaged 7 hours per task. AI-assisted
                                with review was 1.4x faster, 1.6x cheaper. The review step is where value
                                is created.
                            </p>
                        </div>
                    </section>

                    <!-- Module 3: Group Debugging — Real Problems -->
                    <section id="group-debugging">
                        <h2>Module 3: Group Debugging &mdash; Real Problems</h2>
                        <p><strong>Duration:</strong> 40 minutes</p>

                        <p>
                            Participants bring actual tools with actual problems. This module surfaces common failure patterns and builds diagnostic skills. If students don't have broken tools, use the pre-built scenarios below.
                        </p>

                        <h3>Debugging Clinic Protocol</h3>
                        <p>
                            Use this structured protocol for each debugging session:
                        </p>
                        <ol>
                            <li>
                                <strong>Student Presentation (2 minutes):</strong> Student explains what the tool should do, what it actually does, and what they've already tried. Use the format: "Expected behavior... Actual behavior... Steps I've taken..."
                            </li>
                            <li>
                                <strong>Group Diagnosis (3 minutes):</strong> Group asks clarifying questions and proposes hypotheses. Instructor guides with questions: "Is this a data issue or a logic issue? Is the problem at the input stage or the output stage? Have we seen this pattern before?"
                            </li>
                            <li>
                                <strong>Instructor Synthesis (2 minutes):</strong> Instructor identifies the root cause category (frontier limitation, insufficient context, incorrect assumption, integration failure, data quality issue) and explains how to approach the fix.
                            </li>
                            <li>
                                <strong>Document the Pattern:</strong> Add the failure case to the collective frontier map.
                            </li>
                        </ol>

                        <p>
                            <strong>Time allocation:</strong> 7 minutes per problem. Aim for 5 problems in 35 minutes, leaving 5 minutes for final synthesis.
                        </p>

                        <div class="callout callout--instructor">
                            <h3 class="callout__title">Instructor Note: Managing the Session</h3>
                            <p>
                                Keep time strictly. Students will want to fully fix every problem. The goal is not to fix everything; the goal is to build diagnostic patterns. After 3 minutes of group diagnosis, move to synthesis even if the problem isn't solved. Students can continue debugging after class. The value is in recognizing the pattern.
                            </p>
                        </div>

                        <h3>Pre-Built Debugging Scenarios (If Needed)</h3>
                        <p>
                            If students don't bring broken tools, use these three scenarios. Each scenario includes a problem description, expected diagnosis, and solution approach.
                        </p>

                        <div class="template-display">
                            <h4>Scenario 1: Power Automate Flow Triggers But Sends Wrong Data</h4>
                            <p><strong>Situation:</strong> A Power Automate flow is supposed to trigger when a SharePoint list item is updated, then send an email notification to the item creator with the updated information. The flow triggers correctly, but the email always contains the OLD data, not the updated data.</p>
                            <p><strong>Student's Attempted Fixes:</strong> "I rebuilt the email body three times. I checked the SharePoint permissions. I re-authenticated the connection. Nothing works."</p>
                            <details>
                                <summary><strong>Answer Key: Root Cause &amp; Solution</strong></summary>
                                <p><strong>Root Cause:</strong> This is a classic Power Automate timing issue. The trigger "When an item is created or modified" fires immediately when the update is detected, but it captures the item data at the moment of detection, which may be before all fields have finished saving. This is especially common with calculated columns or cascading updates.</p>
                                <p><strong>Diagnosis Questions to Ask:</strong></p>
                                <ul>
                                    <li>Does the email contain the old data or blank data? (Old data suggests timing issue; blank suggests permissions issue)</li>
                                    <li>Are any of the fields calculated or lookup columns? (Calculated columns update asynchronously)</li>
                                    <li>Does the flow work correctly if you manually trigger it 30 seconds after updating? (Confirms timing issue)</li>
                                </ul>
                                <p><strong>Solution Approach:</strong> Add a "Delay" action of 5-10 seconds immediately after the trigger, then use "Get item" to retrieve the updated data explicitly rather than relying on the trigger output. The corrected flow structure: Trigger > Delay (5 seconds) > Get item (by ID) > Send email (using Get item output).</p>
                                <p><strong>Frontier Classification:</strong> This is NOT a frontier issue. This is a platform limitation (Power Automate's trigger timing) that requires domain knowledge to diagnose. AI can generate the flow, but it won't know about this timing quirk unless you tell it.</p>
                            </details>
                        </div>

                        <div class="template-display">
                            <h4>Scenario 2: Power App Form Saves But Doesn't Validate</h4>
                            <p><strong>Situation:</strong> A Power Apps form is supposed to validate that a phone number is in the format XXX-XXX-XXXX before saving to a SharePoint list. The form has a text input field with a validation formula, and the submit button should be disabled if the phone number is invalid. The form saves successfully, but it accepts phone numbers in any format, even completely invalid entries like "abc123".</p>
                            <p><strong>Student's Attempted Fixes:</strong> "I asked the AI to write a validation formula three times. I tested different regex patterns. The formula shows no errors in Power Apps, but it doesn't actually prevent invalid data from being submitted."</p>
                            <details>
                                <summary><strong>Answer Key: Root Cause &amp; Solution</strong></summary>
                                <p><strong>Root Cause:</strong> The validation formula is applied to the text input field, but the submit button's DisplayMode property isn't checking the validation state correctly. Common error: the submit button checks if the field is blank (IsBlank) but doesn't check if the field is valid. The form allows submission because the button logic only checks for presence of data, not correctness of data.</p>
                                <p><strong>Diagnosis Questions to Ask:</strong></p>
                                <ul>
                                    <li>What is the exact formula in the text input's Validation property? (Check if the formula is correct)</li>
                                    <li>What is the DisplayMode property of the submit button? (This is where the actual problem usually is)</li>
                                    <li>Does the text input show a red error indicator when you type an invalid number? (If yes, validation works but button logic is wrong; if no, validation formula is wrong)</li>
                                </ul>
                                <p><strong>Solution Approach:</strong> Set the submit button's DisplayMode to: If(TextInput_Phone.Valid, DisplayMode.Edit, DisplayMode.Disabled). This checks the Valid property of the input field, which reflects the validation formula result. The validation formula itself should be: IsMatch(TextInput_Phone.Text, "^\d{3}-\d{3}-\d{4}$")</p>
                                <p><strong>Frontier Classification:</strong> This is a context issue. AI generated a validation formula, but the student didn't specify that the button's DisplayMode must respect the validation state. This is a common prompting gap: students ask for "validation" but don't specify all the integration points where validation must be enforced.</p>
                            </details>
                        </div>

                        <div class="template-display">
                            <h4>Scenario 3: Dashboard Shows Stale Data</h4>
                            <p><strong>Situation:</strong> A Power BI dashboard pulls from an Excel file stored in SharePoint. When the Excel file is updated, the dashboard doesn't show the new data until the student manually clicks "Refresh" in Power BI Desktop and republishes the report. The student wants the dashboard to automatically update when the source file changes.</p>
                            <p><strong>Student's Attempted Fixes:</strong> "I set up a scheduled refresh in the Power BI service. I checked the data source credentials. I even rebuilt the data connection. The refresh runs successfully according to the logs, but the dashboard still shows old data unless I manually republish from Power BI Desktop."</p>
                            <details>
                                <summary><strong>Answer Key: Root Cause &amp; Solution</strong></summary>
                                <p><strong>Root Cause:</strong> This is a data connection configuration issue. When the Power BI report was created, the data source was set to the local file path (e.g., C:\Users\...\file.xlsx) instead of the SharePoint URL. The scheduled refresh in Power BI Service is trying to refresh from the local path, which doesn't exist in the cloud. The refresh "succeeds" but retrieves no new data because it's looking in the wrong place.</p>
                                <p><strong>Diagnosis Questions to Ask:</strong></p>
                                <ul>
                                    <li>When you created the data connection, did you use "Get Data > SharePoint Folder" or did you download the Excel file and use "Get Data > Excel"? (If the latter, this is the problem)</li>
                                    <li>In Power BI Desktop, go to Transform Data > Data source settings. What does the file path show? (If it shows a C:\ path instead of a SharePoint URL, this confirms the diagnosis)</li>
                                    <li>In the Power BI Service refresh history, does it show any warnings or errors, or does it show "Completed successfully"? (This scenario usually shows "Completed successfully" with zero rows refreshed)</li>
                                </ul>
                                <p><strong>Solution Approach:</strong> Rebuild the data connection using the SharePoint connector. In Power BI Desktop: Get Data > SharePoint Folder > Enter the SharePoint site URL > Navigate to the folder containing the Excel file > Filter to your specific file > Load. Then configure scheduled refresh in Power BI Service using the SharePoint Online credentials. This establishes a cloud-to-cloud connection that can refresh automatically.</p>
                                <p><strong>Frontier Classification:</strong> This is a domain knowledge issue, not a frontier issue. AI can generate the dashboard, but it can't know whether you connected to a local file or a SharePoint URL unless you explicitly describe your connection method. This is a common problem with AI-assisted builds: the AI assumes the "standard" approach, but doesn't know the nuances of your environment.</p>
                            </details>
                        </div>

                        <h3>Final Synthesis (5 minutes)</h3>
                        <p>
                            After all debugging sessions, lead a group discussion:
                        </p>
                        <ul>
                            <li>What patterns did we see? (Common answer: most problems were context issues, not frontier issues)</li>
                            <li>How many problems were caused by insufficient prompting versus actual AI limitations?</li>
                            <li>What questions should we ask the AI differently next time?</li>
                            <li>Which problems belong on the frontier map?</li>
                        </ul>

                        <div class="callout callout--success">
                            <h3 class="callout__title">Key Takeaway</h3>
                            <p>
                                Most debugging comes down to three categories: (1) the AI didn't have enough context, (2) the platform has a quirk the AI doesn't know about, or (3) we hit an actual frontier limitation. Categories 1 and 2 can be solved with better prompting and domain knowledge. Category 3 goes on the frontier map.
                            </p>
                        </div>
                    </section>

                    <!-- Module 5: Teaching Others — The 201 Multiplier -->
                    <section id="teaching-others">
                        <h2>Module 5: Teaching Others &mdash; The 201 Multiplier</h2>
                        <p><strong>Duration:</strong> 30 minutes</p>

                        <h3>The Permission Gap</h3>
                        <p>
                            Mollick's research shows workers already using AI but hiding it.
                            Worried about organizational reaction. This creates a shadow AI culture
                            where best practices aren't shared. Your role as an Advanced Workshop graduate is to formalize AI use, share techniques, and train others.
                        </p>

                        <h3>Discussion: The Apprentice Problem (5 minutes)</h3>
                        <p>
                            Entry-level job postings dropped 35% from 2023 to 2025. AI is automating
                            the routine tasks that juniors traditionally learned on. If a junior never
                            manually writes a key document because AI generates it, how do they develop
                            the judgment to know when the AI-generated version is wrong?
                        </p>

                        <h3>Protocol for Junior Marines Using AI</h3>
                        <ul>
                            <li>
                                <strong>Require review and explanation</strong> &mdash; Juniors must review AI output
                                and explain WHY it is correct or incorrect.
                            </li>
                            <li>
                                <strong>Periodically work without AI</strong> &mdash; Key tasks should periodically
                                be done from scratch to build foundational skills.
                            </li>
                            <li>
                                <strong>Use AI output as a teaching tool</strong> &mdash; Give juniors AI-generated
                                products and have them find the problems.
                            </li>
                            <li>
                                <strong>Rotate QA review</strong> &mdash; Assign juniors to the QA review step so
                                they develop quality judgment through repeated exposure.
                            </li>
                        </ul>

                        <h3>Structured Teach-Back Exercise (20 minutes)</h3>
                        <p>
                            This exercise develops your ability to teach EDD concepts to others. Each student will prepare and deliver a 3-minute teaching segment on one concept from the EDD curriculum.
                        </p>

                        <h4>Step 1: Select a Concept (2 minutes)</h4>
                        <p>Choose one of the following concepts from the curriculum:</p>
                        <ul>
                            <li>Centaur vs. Cyborg modes</li>
                            <li>Frontier mapping</li>
                            <li>Context-building in prompts</li>
                            <li>Iterative refinement</li>
                            <li>Verification protocols</li>
                            <li>The Jagged Frontier</li>
                        </ul>

                        <h4>Step 2: Preparation Using Template (5 minutes)</h4>
                        <p>Use this template to prepare your teaching segment:</p>

                        <div class="template-display">
                            <h5>Teach-Back Preparation Template</h5>
                            <p><strong>Concept:</strong> [Which concept are you teaching?]</p>
                            <p><strong>One-Sentence Definition:</strong> [Define the concept in one clear sentence]</p>
                            <p><strong>Why It Matters:</strong> [One sentence explaining why this concept is important]</p>
                            <p><strong>Real Example from Your Work:</strong> [Specific example from your actual job where this concept applies]</p>
                            <p><strong>Common Mistake:</strong> [One mistake people make when applying this concept]</p>
                            <p><strong>Key Takeaway:</strong> [One sentence the audience should remember]</p>
                        </div>

                        <div class="callout callout--instructor">
                            <h3 class="callout__title">Instructor Note: Template Modeling</h3>
                            <p>
                                Before students prepare, model the template yourself with a completed example. Show them what "good" looks like. Example: "I'm teaching Centaur vs. Cyborg. Definition: Centaur mode means human does some tasks and AI does others separately; Cyborg mode means human and AI work together iteratively on the same task. Why it matters: Using the wrong mode causes either wasted time or quality failures. My example: When I built a training schedule, I used Cyborg mode for the draft but switched to Centaur mode for the final verification because accuracy was critical. Common mistake: People stay in Cyborg mode for accuracy-critical work and don't slow down to verify. Key takeaway: Match the mode to the risk level of the task."
                            </p>
                        </div>

                        <h4>Step 3: Small Group Teaching (10 minutes)</h4>
                        <p>Break into groups of 3-4. Each person delivers their 3-minute teaching segment. Rotate until everyone has taught.</p>

                        <h4>Step 4: Peer Evaluation (3 minutes)</h4>
                        <p>After each teaching segment, group members provide feedback using this rubric:</p>

                        <div class="table-wrapper">
                            <table>
                                <thead>
                                    <tr>
                                        <th scope="col">Criteria</th>
                                        <th scope="col">Strong</th>
                                        <th scope="col">Needs Improvement</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <tr>
                                        <td>Clarity of Definition</td>
                                        <td>I could explain this concept to someone else now</td>
                                        <td>I'm still unclear on what this concept means</td>
                                    </tr>
                                    <tr>
                                        <td>Relevance of Example</td>
                                        <td>The example made the concept concrete and believable</td>
                                        <td>The example was generic or didn't clearly illustrate the concept</td>
                                    </tr>
                                    <tr>
                                        <td>Practical Takeaway</td>
                                        <td>I know what to do differently because of this teaching</td>
                                        <td>I understand the concept but don't know how to apply it</td>
                                    </tr>
                                </tbody>
                            </table>
                        </div>

                        <div class="callout callout--instructor">
                            <h3 class="callout__title">Facilitating the Exercise</h3>
                            <p>
                                Walk the room during small group teaching. Listen for common issues: (1) Students who read from their template instead of teaching conversationally, (2) Examples that are too vague ("I used this on a project" instead of specific details), (3) Students who exceed 3 minutes (this is a teaching skill: brevity). Give real-time coaching. The goal is not perfect teaching; the goal is building awareness of what effective teaching looks like.
                            </p>
                        </div>

                        <h3>Group Debrief (5 minutes)</h3>
                        <p>Reconvene as a full group. Discuss:</p>
                        <ul>
                            <li>What made a teaching segment effective?</li>
                            <li>What was difficult about teaching something you know well?</li>
                            <li>How would you adapt this approach to teach a 30-minute Platform Training session?</li>
                            <li>Who in your unit could benefit from learning these concepts?</li>
                        </ul>

                        <h3>Module 6: Workflow Playbook (20 minutes)</h3>
                        <p>
                            Each participant produces a one-page playbook for one AI-integrated workflow
                            from their actual job. This is the final deliverable of the Advanced Workshop.
                        </p>

                        <div class="callout callout--info">
                            <h3 class="callout__title">Completed Example: Weekly Training Schedule Publication</h3>
                            <p>Use this filled-in example to show participants what a finished playbook looks like before they create their own.</p>
                        </div>
                        <div class="table-wrapper">
                            <table>
                                <thead>
                                    <tr>
                                        <th scope="col">Field</th>
                                        <th scope="col">Content</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <tr>
                                        <td><strong>Task</strong></td>
                                        <td>Weekly training schedule publication for the section</td>
                                    </tr>
                                    <tr>
                                        <td><strong>Frequency</strong></td>
                                        <td>Weekly &mdash; every Thursday by 1600</td>
                                    </tr>
                                    <tr>
                                        <td><strong>Mode</strong></td>
                                        <td>Cyborg (continuous back-and-forth refinement)</td>
                                    </tr>
                                    <tr>
                                        <td><strong>Step 1</strong></td>
                                        <td><strong>Human:</strong> Pull next week&rsquo;s events from training calendar, OPORD, and any new taskings &mdash; <em>Human Only</em></td>
                                    </tr>
                                    <tr>
                                        <td><strong>Step 2</strong></td>
                                        <td><strong>AI:</strong> Draft the schedule in standard weekly format with times, locations, and uniform requirements &mdash; <em>AI generates, Human reviews</em></td>
                                    </tr>
                                    <tr>
                                        <td><strong>Step 3</strong></td>
                                        <td><strong>Human:</strong> Cross-reference against range bookings, vehicle requests, and instructor availability &mdash; <em>Human Only</em></td>
                                    </tr>
                                    <tr>
                                        <td><strong>Step 4</strong></td>
                                        <td><strong>AI:</strong> Format conflicts as a decision matrix: &ldquo;Event A conflicts with Event B at 0900. Options: move A to 1300, move B to Tuesday, or split the section.&rdquo; &mdash; <em>AI generates options, Human decides</em></td>
                                    </tr>
                                    <tr>
                                        <td><strong>Step 5</strong></td>
                                        <td><strong>Human:</strong> Make final decisions on conflicts, add section leader notes &mdash; <em>Human Only</em></td>
                                    </tr>
                                    <tr>
                                        <td><strong>Step 6</strong></td>
                                        <td><strong>AI:</strong> Generate the final formatted schedule with all corrections applied, ready for distribution &mdash; <em>AI generates, Human approves</em></td>
                                    </tr>
                                    <tr>
                                        <td><strong>Verification Checklist</strong></td>
                                        <td>All events have confirmed locations. All times are in 24-hour format. No double-bookings remain. Uniform for each event is specified. POC listed for each event.</td>
                                    </tr>
                                    <tr>
                                        <td><strong>Known Frontier Issues</strong></td>
                                        <td>AI sometimes invents room numbers that don&rsquo;t exist on base. AI cannot verify range availability &mdash; must be checked manually. AI occasionally uses 12-hour time format even when told to use 24-hour.</td>
                                    </tr>
                                    <tr>
                                        <td><strong>Time Savings</strong></td>
                                        <td>Without AI: ~3 hours (gathering info, formatting, resolving conflicts manually). With AI: ~45 minutes (human gathers info, AI formats and generates options).</td>
                                    </tr>
                                    <tr>
                                        <td><strong>Junior Development</strong></td>
                                        <td>Rotate schedule duty among junior Marines weekly. Require the Marine to review AI output and brief back why each event is scheduled (builds planning judgment). Monthly: have one schedule created entirely without AI assistance to maintain baseline skill.</td>
                                    </tr>
                                </tbody>
                            </table>
                        </div>

                        <h4>Blank Template &mdash; Create Your Own</h4>
                        <div class="table-wrapper">
                            <table>
                                <thead>
                                    <tr>
                                        <th scope="col">Field</th>
                                        <th scope="col">Content</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <tr>
                                        <td>Task</td>
                                        <td>A specific, recurring task from your job</td>
                                    </tr>
                                    <tr>
                                        <td>Frequency</td>
                                        <td>How often you perform this task</td>
                                    </tr>
                                    <tr>
                                        <td>Mode</td>
                                        <td>Centaur or Cyborg</td>
                                    </tr>
                                    <tr>
                                        <td>Steps</td>
                                        <td>Step-by-step process with Human/AI labels for each step</td>
                                    </tr>
                                    <tr>
                                        <td>Verification Checklist</td>
                                        <td>What must be checked before output is final</td>
                                    </tr>
                                    <tr>
                                        <td>Known Frontier Issues</td>
                                        <td>Where AI has failed on this task before</td>
                                    </tr>
                                    <tr>
                                        <td>Time Savings Estimate</td>
                                        <td>Time without AI vs. time with AI</td>
                                    </tr>
                                    <tr>
                                        <td>Junior Development Note</td>
                                        <td>How this workflow preserves skill-building for junior personnel</td>
                                    </tr>
                                </tbody>
                            </table>
                        </div>
                    </section>

                    <!-- Assessment Rubric -->
                    <section id="assessment">
                        <h2>Assessment Rubric</h2>
                        <p>
                            Use this rubric to evaluate student performance across all modules. Students should achieve "Meets" or higher in at least 5 of 6 categories to be considered Advanced Workshop graduates.
                        </p>

                        <div class="table-wrapper">
                            <table>
                                <thead>
                                    <tr>
                                        <th scope="col">Criteria</th>
                                        <th scope="col">Exceeds Expectations</th>
                                        <th scope="col">Meets Expectations</th>
                                        <th scope="col">Developing</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <tr>
                                        <td><strong>Frontier Map Completeness</strong><br>(Module 1)</td>
                                        <td>Frontier map covers 5+ categories with specific examples of what AI handles, what it fails at, and what's changing. Map includes evidence from student's own testing. Moving frontier items include planned re-test dates.</td>
                                        <td>Frontier map covers 3-4 categories with clear boundaries. Examples are specific to the student's domain. Distinguishes between "inside" and "outside" frontier accurately.</td>
                                        <td>Frontier map is generic or vague. Categories are not specific to student's domain. Does not clearly identify frontier boundaries or relies on assumptions rather than testing.</td>
                                    </tr>
                                    <tr>
                                        <td><strong>Complex Build Quality</strong><br>(Module 2)</td>
                                        <td>Successfully completed Unit Readiness Dashboard with all three data sources integrated. Made conscious mode-switching decisions and articulated why each phase required that mode. Verified outputs at phase boundaries. Dashboard numbers are accurate when spot-checked against source data.</td>
                                        <td>Completed most of the dashboard build. Made at least 2 mode switches with clear rationale. Attempted verification even if errors were found. Understands the difference between centaur and cyborg modes in practice.</td>
                                        <td>Did not complete the build OR stayed in one mode throughout OR could not articulate why mode-switching matters. Did not verify outputs. Dashboard contains obvious errors not caught in QA.</td>
                                    </tr>
                                    <tr>
                                        <td><strong>Debugging Contribution</strong><br>(Module 3)</td>
                                        <td>Actively diagnosed problems during group debugging. Asked clarifying questions that narrowed down root cause. Correctly identified whether failures were frontier issues, context issues, or platform quirks. Contributed patterns to the collective frontier map.</td>
                                        <td>Participated in group debugging. Asked questions and proposed solutions. Could distinguish between AI limitations and prompting issues when guided by instructor. Documented at least one failure pattern.</td>
                                        <td>Did not actively participate in debugging OR could not identify root causes OR attributed all problems to "AI limitations" without deeper analysis. Did not document patterns.</td>
                                    </tr>
                                    <tr>
                                        <td><strong>QA Protocol Rigor</strong><br>(Module 4)</td>
                                        <td>Identified all 5 errors in the QA exercise in under 10 minutes. Explained why each error is dangerous. Applied the QA checklist to own work and found at least one issue. QA process is systematic and repeatable.</td>
                                        <td>Identified 3-4 errors in the QA exercise. Understands the importance of verification. Can articulate what must be checked in AI-generated output for their domain.</td>
                                        <td>Identified fewer than 3 errors OR took longer than 15 minutes OR did not apply QA thinking to own work. Treats QA as optional rather than critical.</td>
                                    </tr>
                                    <tr>
                                        <td><strong>Teaching Effectiveness</strong><br>(Module 5)</td>
                                        <td>Delivered a clear, concise teach-back with a specific real-world example. Stayed within 3 minutes. Explained not just what the concept is, but why it matters and how to apply it. Received "Strong" ratings on all three rubric criteria from peers.</td>
                                        <td>Delivered a teach-back that communicated the concept clearly. Used a real example. Stayed close to time limit. Received "Strong" on at least 2 of 3 rubric criteria from peers.</td>
                                        <td>Teach-back was unclear, too long, or relied on generic examples. Could not explain how to apply the concept. Did not receive "Strong" on any rubric criteria from peers.</td>
                                    </tr>
                                    <tr>
                                        <td><strong>Workflow Playbook Completeness</strong><br>(Module 6)</td>
                                        <td>Workflow playbook covers a real, recurring task with specific step-by-step details. Each step is labeled Human/AI with rationale. Verification checklist is thorough and testable. Known frontier issues are documented from actual experience. Time savings estimate is backed by real data. Junior development protocol is specific and actionable.</td>
                                        <td>Workflow playbook covers a real task. Steps are clear. Verification checklist is present. Frontier issues are identified. Time savings estimate is reasonable. Junior development note is included.</td>
                                        <td>Workflow playbook is incomplete or generic. Steps are vague. Verification checklist is missing or not specific. No documented frontier issues or time savings. No consideration of junior development.</td>
                                    </tr>
                                </tbody>
                            </table>
                        </div>

                        <div class="callout callout--info">
                            <h3 class="callout__title">Using This Rubric</h3>
                            <p>
                                This rubric is not a checklist. Use it as a guide for observing student performance throughout the workshop. The most important indicator of success is whether students demonstrate conscious decision-making about when and how to use AI. A student who completes all deliverables but cannot articulate their reasoning has not achieved the learning objectives. Conversely, a student who struggles with technical execution but shows strong diagnostic thinking and mode-switching awareness is on the right track.
                            </p>
                        </div>

                        <h3>Certification Recommendation</h3>
                        <p>
                            Students who achieve "Meets" or higher in at least 5 of 6 categories are recommended for:
                        </p>
                        <ul>
                            <li>Serving as Platform Training instructors</li>
                            <li>Leading tool development projects in their units</li>
                            <li>Mentoring junior personnel in AI-assisted workflows</li>
                            <li>Contributing to frontier map updates and workflow documentation</li>
                        </ul>

                        <p>
                            Students who do not meet this threshold should be encouraged to continue practicing, revisit specific modules, and attempt Advanced Workshop again after building 1-2 additional tools.
                        </p>
                    </section>

                </div>
            </div>
        </div>
    </main>

    <!-- Site Footer -->
    <footer class="site-footer" role="contentinfo">
        <div class="container">
            <div class="footer-bottom">
                <span>Expert-Driven Development</span>
                <span>UNCLASSIFIED</span>
                <span>MIT License</span>
                <span>SSgt Jesse C. Morgan, MCD-Monterey</span>
            </div>
        </div>
    </footer>

    <script>
        // Mobile navigation toggle
        (function() {
            var toggle = document.querySelector('.nav-toggle');
            var mobileNav = document.getElementById('mobile-nav');
            if (toggle && mobileNav) {
                toggle.addEventListener('click', function() {
                    var expanded = this.getAttribute('aria-expanded') === 'true';
                    this.setAttribute('aria-expanded', String(!expanded));
                    mobileNav.classList.toggle('is-open');
                });
            }
        })();
    </script>

</body>
</html>

# Expert-Driven Development — Training Curricula v5.0
## Revised February 2026

### What Changed From v4.0

The v4.0 curriculum restructured training around six 201-level skills. That was the right move. But it crammed two different audiences into the same opening course: people who need to *use* AI effectively and people who want to *build* tools with AI. Those are different training objectives for different populations.

This revision adds a fifth course — AI Fluency Fundamentals — as a standalone 2-hour universal requirement. Every Marine, sailor, and civilian at MCCES takes this course. It teaches the six 201-level skills, the jagged frontier, centaur/cyborg work patterns, and quality judgment. No tool-building required. No Power Platform. Just the applied judgment skills that research shows actually predict sustained AI adoption.

The builder courses (Trainings 1–3) now assume students arrive with the 201 framework already internalized. They spend zero time on theory and 100% of time building.

### Why This Matters Now

The DoW's January 2026 AI Strategy declares that "2026 will be the year we emphatically raise the bar for Military AI Dominance." The Army stood up a dedicated AI/ML officer career field (49B) in October 2025. The Pentagon launched GenAI.mil in December 2025. The Marine Corps is hosting generative AI workshops at Quantico. Every service is scrambling to build AI fluency across the force.

But fluency is not the same as access. Giving people AI tools without teaching them judgment is worse than giving them nothing — it's the jagged frontier problem at institutional scale. This curriculum fills the gap between "here's a tool" and "here's how to think."

---

## The Research Foundation

This curriculum is not built on intuition. It's built on the largest studies of AI and work ever conducted. Every design decision traces back to empirical findings.

### Study 1: The 80% Abandonment Problem
**Source:** Microsoft Work Trend Index, enterprise deployment data across 300,000+ employees

Microsoft tracked AI tool adoption at enterprise scale. Excitement peaked for approximately three weeks after deployment. Then came what researchers called a "crater of disappointment." The vast majority of workers quietly stopped using AI tools. The survivors didn't write better prompts — they learned to manage AI like a capable but inexperienced team member.

**Design implication:** Our curriculum opens with this finding. The first thing every student hears is that most people fail, and the reason is not the technology — it's the approach. Training must teach management skills, not tool skills.

### Study 2: The Jagged Frontier
**Source:** Dell'Acqua et al. (2023). "Navigating the Jagged Technological Frontier." Harvard Business School Working Paper. 758 BCG consultants.

When consultants used AI on tasks inside its capability boundary, they completed 12.2% more tasks, 25.1% faster, with 40% higher quality. But on tasks outside that boundary — tasks that *looked* like AI should handle them — consultants were 19 percentage points less likely to produce correct solutions than those working without AI at all.

The study also identified two productive work patterns: **Centaurs** (clear human/AI division of labor) and **Cyborgs** (fluid, continuous human/AI integration). Both patterns outperformed unstructured AI use.

**Design implication:** Frontier Recognition is a core 201 skill, not a footnote. We teach students to identify the boundary and share failure cases. The centaur/cyborg framework gives students a vocabulary for choosing how to work with AI on different task types.

### Study 3: The Skill-Leveling Effect
**Source:** Brynjolfsson, Li, & Raymond (2025). "Generative AI at Work." *Quarterly Journal of Economics*, 140(2), 889–942. 5,172 customer support agents.

AI assistance increased productivity by 15% on average, but the gains were dramatically uneven. Novice and low-skilled workers improved by 34%. The most experienced workers saw minimal gains and slight quality decreases. AI effectively disseminated the tacit knowledge of top performers to everyone else. Workers with two months of AI-assisted experience performed as well as workers with six months of unassisted experience.

**Design implication:** AI doesn't replace expertise — it amplifies it. But it amplifies judgment *and* errors equally. This is why Quality Judgment is the most important 201 skill: the same tool that makes a junior Marine more productive also makes an overconfident one more dangerous.

### Study 4: Management as the Core AI Skill
**Source:** Mollick (2026). "Management as AI Superpower." *One Useful Thing.* Wharton executive MBA field experiment.

Mollick's January 2026 experiment put non-technical executive MBA students through a four-day AI-intensive startup exercise. Students with management experience — the ability to scope problems, define deliverables, give feedback, and recognize when output is wrong — dramatically outperformed those without, regardless of technical ability. The key insight: AI work is delegation work. The skills are the same: clear instructions, quality evaluation, iterative feedback, and knowing what "done" looks like.

Mollick also formalized the **Equation of Agentic Work**: the decision to delegate to AI depends on (1) how long the task takes you to do, (2) how likely the AI is to succeed, and (3) how long it takes to evaluate the AI's output. Subject matter expertise improves all three variables.

**Design implication:** Our entire curriculum is built on the management framing. "You're not learning to use a tool. You're learning to manage one." This framing also validates the military audience: Marines already know how to manage. They just need to apply those skills to AI.

### Study 5: The GDPval Expert Parity Benchmark
**Source:** OpenAI (2025). "GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks." 1,320 tasks across 44 occupations.

For the first time, frontier AI models were tested head-to-head against human experts across real-world professional tasks spanning nine GDP-driving industries. Expert graders conducted blind evaluations. The best models produced work rated as good as or better than human experts on nearly half of all tasks. Human experts averaged 7 hours per task; AI completed them in minutes but required approximately one hour of human review. When paired with human oversight, AI-assisted workflows were 1.4x faster and 1.6x cheaper than unassisted experts.

**Design implication:** AI is already at or near expert-level on many knowledge work tasks. The bottleneck is not AI capability — it's human ability to evaluate, direct, and integrate AI output. This validates our entire focus on judgment over prompting.

### Study 6: The UK Government Scale Deployment
**Source:** UK Government Digital Services (2025). Microsoft 365 Copilot deployment across 20,000 government employees in 12 departments.

Government workers reported saving more than 25 minutes per day — nearly two weeks per year. Over 70% said AI helped them spend less time on routine tasks and more time on strategic work. Over 80% said they wouldn't want to give up AI access. Nine of twelve departments continued their licenses after the trial. Results were realized in just three months, consistent with Microsoft's finding that it takes up to 11 weeks to build the AI habit.

**Design implication:** Government bureaucracies — the closest analog to military organizations — see measurable gains when AI is deployed with proper onboarding support. But "support" means training, community sessions, and workshops, not just a login and a tip sheet.

### Study 7: The Apprenticeship Crisis
**Source:** Multiple studies including Brynjolfsson et al. (2025), Stanford Digital Economy Lab analysis, CNBC/Mollick/Kinder reporting (2025).

Entry-level job postings dropped 35% from January 2023 to June 2025 in AI-exposed occupations. Employment among workers aged 22-25 in these fields declined 13%. When AI handles the routine tasks that junior workers traditionally learned on, the apprenticeship pipeline breaks. Organizations that cut junior roles today face a catastrophic expertise shortage in five to ten years — "architects who have never laid a brick."

Mollick's direct warning: "That training pipeline that was always implicit has broken, and it has to be reconstructed."

**Design implication:** Our Supervisor Orientation course explicitly addresses the apprentice problem. Leaders must ensure junior Marines still develop judgment through reviewing AI output, not just accepting it. The curriculum includes specific protocols for preserving developmental opportunities alongside AI efficiency gains.

---

## The Six 201-Level Skills

These six skills are the backbone of the entire curriculum. Every module in every course maps back to at least one of them. None of them are prompting techniques.

| # | Skill | What It Means | 101 Behavior | 201 Behavior |
|---|-------|---------------|--------------|--------------| 
| 1 | Context Assembly | Knowing what information to provide, from which sources, and why | Dumps entire documents or provides almost no context | Curates background, constraints, and examples to improve output quality |
| 2 | Quality Judgment | Knowing when to trust AI output and when to verify | Accepts first output as-is or abandons entirely | Knows which task types need verification, spots reliable vs. unreliable content within the same output |
| 3 | Task Decomposition | Breaking work into AI-appropriate chunks | Throws entire task at AI or avoids it | Identifies which subtasks to delegate vs. retain, like managing a team member |
| 4 | Iterative Refinement | Moving from 70% to 95% through structured passes | Accepts first draft or gives up | Treats first output as starting point, knows how to direct revision |
| 5 | Workflow Integration | Embedding AI into how work actually gets done | AI is a side activity ("I'll try the AI thing later") | AI is an integrated capability ("This is just how we do correspondence now") |
| 6 | Frontier Recognition | Knowing when you're outside AI's capability boundary | Assumes AI is uniformly good or uniformly bad | Knows where AI excels vs. fails for their specific work, shares failure cases |

### Two Work Patterns: Centaur and Cyborg

The BCG-Harvard study identified two productive patterns for working with AI. Both work. The 201 skill is knowing which pattern fits which task.

**Centaur Mode:** Clearly divide work between human and AI. The human does strategy and framing, AI generates options and drafts. Distinct responsibilities with clear handoff points.
- Best for: High-stakes work requiring clear accountability and verification checkpoints
- DoW examples: Legal reviews, admin separations, security clearance processing, fitrep drafting

**Cyborg Mode:** Completely integrate AI into the workflow. Continuous back-and-forth. The boundary between human and AI work becomes fluid.
- Best for: Creative and iterative work where continuous refinement improves output
- DoW examples: Building PowerApps, developing training materials, writing SOPs, data analysis

The mistake is picking one pattern and applying it to everything. Teach both. Let people match the pattern to the task.

### The Jagged Frontier

AI capabilities are uneven across tasks, and the boundary is not intuitive. The BCG-Harvard study found that untrained AI users don't just miss gains — they actively degrade their own work quality on certain tasks. The frontier is also constantly shifting as models improve.

This means Frontier Recognition can't be learned once and memorized. It has to be practiced continuously, and failure cases must be shared. When someone discovers a task AI handles poorly, that knowledge is as valuable as discovering what AI handles well.

---

## Training Overview

### Five Courses, One Goal

| Course | Duration | Audience | Prerequisite | Outcome |
|--------|----------|----------|--------------|---------|
| **AI Fluency Fundamentals** | 2 hours | All personnel | None | Understand the six 201 skills, recognize the jagged frontier, know when to trust AI output, map AI into your workflow |
| **Builder Orientation** | 2 hours | Aspiring builders | AI Fluency Fundamentals | Build a working prototype, apply task decomposition and iterative refinement in practice |
| **Platform Training** | 4 hours | Builders | Builder Orientation | Build 3 complete tools on Power Platform using centaur and cyborg work patterns |
| **Advanced Workshop** | 4 hours | Experienced builders | At least one deployed tool | Map the frontier for your domain, build verification protocols, teach others |
| **Supervisor Orientation** | 30 minutes | Leadership | None | Evaluate proposals, create permission culture, understand the apprentice problem |

### Prerequisites

**AI Fluency Fundamentals:** None. Come as you are.

**Builder Orientation:** Must have completed AI Fluency Fundamentals (in-person or online). Come with a problem you want to solve.

**Platform Training:** Must have completed Builder Orientation. Have M365 account access.

**Advanced Workshop:** Must have built at least one working tool.

**Supervisor Orientation:** None. Can be taken independently. Recommended before or alongside AI Fluency Fundamentals.

---

## Instructor Certification

### Who Can Teach

| Course | Instructor Requirements |
|--------|------------------------|
| AI Fluency Fundamentals | Completed AI Fluency Fundamentals AND at least one builder course, OR demonstrated equivalent expertise |
| Builder Orientation | Completed all builder courses AND deployed at least one tool |
| Platform Training | Same as Builder Orientation, plus demonstrated proficiency with Power Platform |
| Advanced Workshop | Same as Platform Training, plus served as QA reviewer for at least 2 tools |
| Supervisor Orientation | Any qualified AI Fluency Fundamentals instructor |

### Certification Process

1. Complete the relevant course(s) as a student
2. Build and deploy at least one tool under the EDD SOP (for builder courses)
3. Shadow an existing instructor for one delivery of the course
4. Deliver the course with an existing instructor present (co-teach)
5. Receive certification from Program Coordinator

### Train-the-Trainer Guidance

For new units without certified instructors:
- Request support from an established unit's instructor
- Use video recordings of course delivery (if available)
- Program Coordinator may certify based on equivalent experience

**Succession planning:** Each unit should maintain at least 2 certified instructors. When an instructor PCSs, ensure replacement is certified before departure.

### Maintaining Certification

Instructors remain certified as long as they:
- Deliver at least one course per year
- Stay current with platform and AI tool changes
- Maintain active involvement in the developer community

---

## Course 1: AI Fluency Fundamentals

**Required | 2 Hours | All Personnel**

**Purpose:** Give every Marine, sailor, and civilian the applied judgment skills to use AI effectively in their daily work — regardless of whether they ever build a tool. This is the universal prerequisite. Everyone takes it.

**This is not a tool tour.** There are no prompting formulas. No "top 10 things ChatGPT can do." This course teaches the management skills that separate the 20% who sustain AI adoption from the 80% who quit.

### Agenda

| Time | Module | 201 Skills Targeted |
|------|--------|---------------------|
| 0:00–0:15 | Why 80% Quit (and How You Won't) | Frontier Recognition |
| 0:15–0:40 | The Six Skills That Actually Matter | All six — overview |
| 0:40–0:55 | The Delegation Equation | Task Decomposition, Context Assembly |
| 0:55–1:05 | Break | |
| 1:05–1:30 | The Trust Problem: Quality Judgment | Quality Judgment, Frontier Recognition |
| 1:30–1:45 | Your Workflow: Centaur, Cyborg, or Neither | Workflow Integration |
| 1:45–2:00 | Frontier Mapping and Your Assignment | Frontier Recognition, all six |

### Module 1: Why 80% Quit (15 min)

**Opening hook:** Microsoft tracked hundreds of thousands of employees using AI tools. Excitement peaked for about three weeks. Then came a crater of disappointment. The vast majority quietly stopped. The 20% who survived didn't write better prompts — they learned to manage AI like a capable but inexperienced team member.

**Why people fail:**
- They type "help me with this report" and get something generic
- They try again and get something confident and wrong
- They try a third time and decide it's faster to do it themselves
- They never come back

**The punchline:** The UK government gave AI tools to 20,000 workers across 12 departments with proper training and support. After three months, over 80% didn't want to give them up. Workers reported saving 25 minutes a day — nearly two weeks a year. Nine of twelve departments kept their licenses. The difference wasn't the tool. It was the training.

**Why this training is different:** We're not teaching you how to use a tool. We're teaching you how to manage one. The same skills that make you a good leader — breaking work into pieces, knowing what good looks like, giving constructive feedback, verifying quality — are the skills that make you effective with AI.

**The management framing:** Would you hand a 100-page RFP to a brand new Marine and say "handle this"? No. You'd break it into pieces. You'd tell them which parts to tackle first. You'd explain what good looks like. You'd review their work and give feedback. That's how you should work with AI.

**Instructor note:** Cite the exact numbers but keep the energy conversational. This is not a PowerPoint lecture. It's a wake-up call. The point is: most people fail at AI not because they're dumb, but because nobody taught them the right skills. You're about to learn the right skills.

### Module 2: The Six Skills That Actually Matter (25 min)

Walk through all six 201 skills with military-relevant examples. For each skill, show the 101 behavior and the 201 behavior side by side.

**Skill 1 — Context Assembly:**
- 101: "Write me a counseling statement."
- 201: "Write a counseling statement for a Lance Corporal who was late to formation twice this month. The tone should be corrective but not adversarial. The Marine has otherwise been a solid performer. Use the format from NAVMC 10274. This is a page 11 entry, not an adverse 6105."

**Key teaching point:** AI is extremely sensitive to context quality. The difference between mediocre output and useful output is usually not the prompt structure — it's the information you provide. This is exactly like giving a task to a new join: the more specific your guidance, the better the result.

**Skill 2 — Quality Judgment:**
- Show an AI-generated naval message with three correct elements and two subtle errors (wrong DTG format, incorrect reference number)
- Ask: "Would you sign this?" Then reveal the errors.

**Key teaching point:** AI can be confidently correct and confidently wrong in the same paragraph. The 201 skill is spotting which is which. The Brynjolfsson study found that AI helped novice workers improve by 34% — but only because the AI was disseminating expert-level patterns. If you can't judge quality, you're amplifying errors, not expertise.

**Skill 3 — Task Decomposition:**
- 101: "Write me a training plan for the platoon."
- 201: Break it into subtasks. "First, list the required annual training events for FY26. Then map them against the calendar, avoiding known conflicts. Then identify which events need range time and put those first."

**Key teaching point:** Mollick calls this the Equation of Agentic Work. You're deciding whether to delegate based on three things: how long it would take you, how likely AI is to get it right, and how long it takes you to check. Breaking tasks into smaller pieces improves the AI's success probability on each piece.

**Skill 4 — Iterative Refinement:**
- Show a real first draft from AI. It's 70% there. Then demonstrate the iteration:
  - "The tone is too formal. Make it sound like a Gunny talking to his Marines."
  - "The third paragraph repeats the first. Consolidate."
  - "Add specific dates from this schedule: [paste schedule]."
- Three passes. Now it's 95%.

**Key teaching point:** The first output is never the final product. It's a starting point you manage toward completion. You wouldn't accept an intern's first draft — don't accept AI's. The GDPval study found that simple improvements to prompts could boost AI win rates by another five percentage points. Iteration isn't a workaround — it's the skill.

**Skill 5 — Workflow Integration:**
- Side activity: "I'll try using AI for this one thing later."
- Integrated: "Every Friday, the duty NCO uses AI to draft the weekend safety brief from the current weather, liberty boundaries, and recent incidents. That's just how we do it now."

**Key teaching point:** AI becomes valuable when it's embedded in a recurring workflow, not used as an occasional experiment. The goal is a playbook that anyone in the section can follow. Microsoft's research shows it takes up to 11 weeks to build the AI habit. If you only use it once, you'll never get past the crater of disappointment.

**Skill 6 — Frontier Recognition:**
- Show examples of tasks AI handles well in your domain (formatting, drafting, summarizing, data organization)
- Show examples of tasks AI handles poorly (interpreting specific MCOs, calculating TIS/TIG accurately, anything requiring institutional knowledge not in training data)
- Introduce the jagged frontier: the capability boundary is not intuitive, and it changes as models improve

**Key teaching point:** The BCG-Harvard study found that untrained users working outside the frontier didn't just fail to benefit — they performed 19 percentage points *worse* than those without AI. When you discover a task AI handles poorly, share it. That knowledge is as valuable as discovering what AI handles well.

### Module 3: The Delegation Equation (15 min)

**Mollick's framework:** For any task, ask three questions:
1. **Human Baseline Time** — How long would this take me to do myself?
2. **Probability of Success** — How likely is AI to produce acceptable output?
3. **AI Process Time** — How long does it take me to request, wait, and evaluate?

**Walk through examples:**
- Writing a 5-paragraph order from scratch: 4 hours human time. AI can draft in minutes. But reviewing for accuracy in a 5-paragraph format with specific unit details takes 45 minutes. If AI gets it 70% right, that's still a significant time save — delegate.
- Calculating a Marine's TIS/TIG for promotion: 5 minutes human time with the right references. AI might get it right, might not. Checking takes as long as doing it. Don't delegate.
- Drafting a quarterly training schedule: 6 hours human time. AI can produce a solid first draft in minutes if given the right constraints. 30 minutes to review and adjust. High probability of acceptable output on the structure, lower on specific details. Delegate the structure, fill in the specifics yourself.

**Key teaching point:** The more expertise you have, the better you are at all three variables. Experts give better instructions (higher probability of success), spot errors faster (lower AI process time), and know which tasks are worth delegating (better human baseline estimates). AI doesn't replace expertise. It rewards it.

### Module 4: The Trust Problem — Quality Judgment (25 min)

**THE RED PEN REVIEW EXERCISE**

This is the highest-value activity in the entire AI Fluency Fundamentals course.

**Setup:** Distribute three AI-generated documents relevant to your unit's work:
1. A leave request endorsement (mostly correct, one procedural error)
2. A situation report summary (accurate data, wrong attribution in one place)
3. A training evaluation write-up (good structure, fabricated reference number)

**Exercise:** Participants have 10 minutes. Red pen each document. Mark anything you wouldn't sign.

**Debrief (15 min):**
- Reveal what was correct and what was wrong
- Key question: "Did you catch the fabricated reference number?" Most people don't. AI doesn't flag uncertainty — it presents everything with equal confidence.
- Introduce the verification hierarchy:
  - **High-stakes documents** (legal, personnel, financial): Line-by-line verification of every fact, reference, and number. Centaur mode — human verifies everything.
  - **Medium-stakes documents** (correspondence, reports, briefings): Spot-check key claims, verify references, check tone. Mixed mode.
  - **Low-stakes documents** (first drafts, brainstorming, formatting): Quick review for obvious errors. Cyborg mode.
- Connect to the Brynjolfsson finding: AI improved novice workers by 34% because it disseminated expert patterns. But when experienced workers followed AI suggestions uncritically, quality declined slightly. *You* are the quality control.

**Instructor note:** This exercise is not optional. This is where the training pays for itself. Every participant should leave this module viscerally understanding that AI output requires judgment, not just acceptance. Make it sting a little — use documents from their actual domain where the errors would matter.

### Module 5: Your Workflow — Centaur, Cyborg, or Neither (15 min)

**Introduce the two patterns:**
- **Centaur:** Clear division. Human does Phase 1, hands off to AI for Phase 2, human reviews in Phase 3. Best for high-stakes work. Think: legal review, admin separation packages, fitness report drafts.
- **Cyborg:** Continuous integration. Human and AI work in constant back-and-forth. The boundary is fluid. Best for creative and iterative work. Think: drafting SOPs, building training materials, analyzing data.

**Exercise: Workflow Mapping (10 min)**

Each participant picks one recurring task from their actual job. On the provided worksheet, they:
1. Break it into 3–5 subtasks
2. Mark each subtask: **Human Only** / **AI Could Help** / **AI Should Do This**
3. Identify which pattern fits: Centaur or Cyborg
4. Estimate time savings if AI handled the appropriate subtasks

**Debrief:** Ask 2–3 volunteers to share their maps. Point out patterns. Many people will find that 30-50% of their recurring work has AI-appropriate subtasks they've never tried delegating.

**Key teaching point:** The UK government study found that trained workers saved 25 minutes a day. That's not from one big win — it's from dozens of small workflow integrations. The compounding effect of AI fluency is more powerful than any single use case.

### Module 6: Frontier Mapping and Your Assignment (15 min)

**Introduce frontier mapping:**
Start a shared document (can be as simple as a whiteboard or a shared spreadsheet) with three columns:
- **AI Handles Well:** Tasks where AI consistently produces good output in your domain
- **AI Handles Poorly:** Tasks where AI fails or produces unreliable output
- **Moving Frontier:** Tasks where AI capability is improving — check periodically

**Seed the map** with 3–5 examples from the instructor's own experience. Then ask: based on what you've seen today, what would you add?

**Assignment:**
1. Pick one recurring task from your workflow map (Module 5)
2. Try using AI on it this week
3. Note what worked and what didn't
4. Bring your results — especially your failure cases — to share with your section

**For those continuing to Builder Orientation:** Bring a specific problem you want to build a tool to solve. Be ready to decompose it into subtasks before you open any software.

**Closing:** You are now equipped with the same judgment framework that separates sustained AI adopters from the 80% who quit. The tools will change. The models will improve. These six skills will still be the difference between productive AI use and wasted time.

---

## Course 2: Builder Orientation

**Elective | 2 Hours | Aspiring Builders**

**Purpose:** For people who want to go beyond daily AI use and actually build tools. Focuses entirely on hands-on building. The 201 framework was covered in AI Fluency Fundamentals — here we apply it.

**Prerequisite:** AI Fluency Fundamentals (in-person or online completion)

### Agenda

| Time | Module | 201 Skills Applied |
|------|--------|---------------------|
| 0:00–0:10 | From User to Builder (bridge from Course 1) | Task Decomposition |
| 0:10–0:40 | Live Build #1: Equipment Tracker | Task Decomposition, Context Assembly, Iterative Refinement |
| 0:40–0:50 | Break | |
| 0:50–1:25 | Live Build #2: When Something Breaks | Iterative Refinement, Quality Judgment |
| 1:25–1:50 | Your Problem: Decomposition Exercise | Task Decomposition, Frontier Recognition |
| 1:50–2:00 | Assignment and Path Forward | All six |

### Module 1: From User to Builder (10 min)

**Bridge from AI Fluency Fundamentals:** You've learned to manage AI as a user — giving it tasks, evaluating output, integrating it into workflows. Building tools is the same skills at higher complexity. Instead of managing one conversation, you're managing an entire project through AI.

**The builder mindset:** Before you open any tool, you decompose on the whiteboard. What are the components? What does the data look like? What does the user need to see? This is Task Decomposition applied to system design.

**Instructor note:** Quickly verify that all students completed AI Fluency Fundamentals. Ask: "What are the six 201 skills?" If students can't recall at least four, do a 2-minute refresher. Then move on.

### Module 2: Live Build #1 — Equipment Tracker (30 min)

**The problem:** "I need a way to track equipment checkout for my section."

**Step 1: Decompose BEFORE touching AI (5 min, whiteboard)**
- What data fields do we need? (item, serial number, person, date out, date due, status)
- What does the user need to do? (check out, check in, see what's overdue)
- What's the simplest version that's useful?

**Step 2: Build with AI (20 min)**
- Instructor narrates every decision: "I'm giving the AI context about our data structure first. Watch how I break this into specific requests rather than one giant ask."
- Build a working equipment tracker in Power Apps
- Demonstrate Context Assembly in real time: "Notice I'm telling it *why* we need the due date field — so it knows to build an overdue alert."
- Demonstrate Iterative Refinement: The first version will be functional but ugly. Run two revision passes.

**Step 3: Review (5 min)**
- Show the finished product
- Ask: "What would you change for your section?"
- Emphasize: This took 30 minutes. Before AI, this would have been a week-long project requiring a developer.

### Module 3: When Something Breaks (35 min)

**Purpose:** Intentionally create a failure and demonstrate debugging with AI.

**Setup:** Continue building on the equipment tracker. Add a feature that's slightly outside AI's comfort zone — perhaps a conditional formatting rule that requires specific business logic.

**Let it break.** When it does:
1. Read the error message out loud
2. Show the debugging process: "I'm going to paste this error back to AI with context about what I was trying to do."
3. Demonstrate that debugging IS the work. This is normal.
4. Show recovery: how the fix often teaches you something about the tool

**Key teaching point:** The first time something breaks, most people quit. That's why they're in the 80%. Debugging with AI is a skill. The AI is better at diagnosing its own errors than you are, but only if you give it context about what went wrong.

**Extended build:** Add 1–2 more features to the tracker. By the end, students see a full cycle: decompose → build → break → debug → improve.

### Module 4: Your Problem — Decomposition Exercise (25 min)

**Exercise:** Each participant takes the problem they brought to class and:
1. Writes it as a one-sentence problem statement
2. Decomposes it into 4–6 subtasks on paper
3. Marks each subtask: **Human designs** / **AI builds** / **Human verifies**
4. Identifies which pattern (centaur or cyborg) fits each phase
5. Identifies potential frontier issues — where might AI struggle with this specific problem?

**Peer review (10 min):** Pair up. Review each other's decomposition. Ask: "Is any subtask too big? Is there a frontier risk you missed?"

**Instructor note:** Walk the room during this exercise. The quality of decomposition predicts the quality of the eventual build. Catch "dump the whole thing on AI" plans early and redirect.

### Module 5: Assignment and Path Forward (10 min)

**Assignment before Platform Training:**
1. Attempt to build your decomposed solution using AI
2. Document what worked and what didn't
3. Identify at least one failure case to share at the next session
4. Bring your working (or partially working) prototype to Platform Training

**Path forward:**
- Platform Training (4 hours): Build three complete tools
- Advanced Workshop (4 hours): Frontier mapping, verification protocols, teaching others
- Ongoing: Contribute to the shared frontier map, submit tools for QA review, mentor others

---

## Course 3: Platform Training

**Elective | 4 Hours | Builders**

**Purpose:** Build three complete tools on Power Platform, practicing both centaur and cyborg work patterns. This is where EDD students become capable of producing deployable tools under the EDD SOP.

**Prerequisite:** Builder Orientation. M365 account access.

### Agenda

| Time | Module | 201 Skills Applied |
|------|--------|---------------------|
| 0:00–0:15 | Setup and Review | All six (quick check) |
| 0:15–1:15 | Build #1: Structured Workflow (Centaur Mode) | Context Assembly, Quality Judgment |
| 1:15–1:30 | Break — Failure Sharing | Frontier Recognition |
| 1:30–2:30 | Build #2: Iterative Application (Cyborg Mode) | Iterative Refinement, Workflow Integration |
| 2:30–2:45 | Break | |
| 2:45–3:45 | Build #3: Your Problem (Choose Your Mode) | Task Decomposition, all six |
| 3:45–4:00 | Frontier Map Update and Wrap | Frontier Recognition |

### Module 1: Setup and Review (15 min)

Verify all participants have M365 access and can reach Power Platform. Quick review of centaur vs. cyborg patterns. Ask 2–3 students to share a failure case from their Builder Orientation assignment.

### Module 2: Build #1 — Structured Workflow, Centaur Mode (60 min)

**The problem:** A request routing workflow — someone submits a request, it goes to the right approver, the approver acts on it, the requester gets notified.

**Explicit centaur pattern:**
- **Phase 1 — Human designs:** Define the approval chain, business rules, and notification requirements on the whiteboard. No AI yet.
- **Phase 2 — AI builds:** Feed the design to AI. Build the Power App form and the Power Automate flow.
- **Phase 3 — Human verifies:** Test every path. Does the right person get notified? What happens with edge cases?
- **Phase 4 — AI refines:** Feed test results back to AI for fixes.
- **Phase 5 — Human accepts:** Final review before deployment.

**Key teaching point:** At every phase boundary, there's a verification checkpoint. The human is responsible for *design* and *verification*. AI handles *execution*. This is the pattern for any high-stakes tool.

### Module 3: Failure Sharing (15 min, during break)

**This is not optional. This is one of the most important 15 minutes in the entire program.**

Structure:
- Each participant shares: what AI got wrong, how they caught it, how they fixed it
- Document on a whiteboard or shared doc
- Look for patterns: Are certain types of tasks consistently problematic?
- This becomes the beginning of the unit's frontier map

**Instructor note:** Create psychological safety. Frame failures as valuable data, not embarrassments. "Every failure case you share saves someone else an hour of confusion." Lead by sharing your own failure cases first.

### Module 4: Build #2 — Iterative Application, Cyborg Mode (60 min)

**The problem:** A training tracker that pulls data from multiple sources and generates reports.

**Explicit cyborg pattern:**
- No distinct phases. Continuous back-and-forth with AI.
- Start with a rough idea: "I need to see which Marines have completed which training events."
- Build incrementally: data model → input form → dashboard → reports
- At each step, evaluate and redirect: "That chart doesn't show what I need. Change the X axis to show completion by date, not by person."
- The boundary between human and AI contribution is fluid. Sometimes you type code directly. Sometimes AI generates entire components.

**Key teaching point:** Cyborg mode is faster but requires more constant attention. It's not "set and forget." It's a continuous conversation. Best for work where you're discovering requirements as you build.

### Module 5: Build #3 — Your Problem, Choose Your Mode (60 min)

Each participant builds the solution they decomposed in Builder Orientation.

**Before starting, each participant states:**
1. What they're building (one sentence)
2. Which mode they're using (centaur or cyborg) and why
3. Where they expect frontier issues (what might AI struggle with)

**During the build:** Instructor circulates, helping with blocks and noting common patterns.

**Instructor note:** Resist the urge to solve every problem. When a student hits a block, ask: "What would you tell the AI about what's going wrong?" Teaching them to debug with AI is more valuable than fixing it yourself.

### Module 6: Frontier Map Update and Wrap (15 min)

Compile all failure cases from today into the shared frontier map:

| Task Type | AI Handles Well | AI Handles Poorly | Verification Needed |
|-----------|----------------|-------------------|---------------------|

This map lives on the EDD site. It grows with every training delivery and every new tool built. Share it with GySgt Swickard for MCCES-wide distribution.

**Assignment before Advanced Workshop:**
- Complete your Build #3 tool to deployable state
- Run it through the EDD SOP QA process
- Document three specific failure cases you encountered
- Identify one area where AI capability surprised you (better or worse than expected)

---

## Course 4: Advanced Workshop

**Elective | 4 Hours | Experienced Builders**

**Purpose:** Move from individual capability to organizational capability. Map the frontier for your functional area, build verification protocols, and learn to teach others.

**Prerequisite:** At least one deployed tool.

### Agenda

| Time | Module | 201 Skills Applied |
|------|--------|---------------------|
| 0:00–0:45 | Frontier Mapping for Your Domain | Frontier Recognition |
| 0:45–1:45 | Complex Build: Multi-Component System | All six |
| 1:45–2:00 | Break | |
| 2:00–2:45 | Verification Protocols and QA | Quality Judgment |
| 2:45–3:30 | Group Debugging: Real Problems | Iterative Refinement, Frontier Recognition |
| 3:30–4:00 | Teaching Others: The 201 Multiplier | All six |

### Module 1: Frontier Mapping for Your Domain (45 min)

Each participant (or working group from the same functional area) creates a frontier map specific to their domain.

**Format:**

| Category | Inside Frontier (AI Handles) | Outside Frontier (AI Fails) | Moving Frontier (Check Periodically) |
|----------|-----------------------------|-----------------------------|--------------------------------------|

**Categories to map:**
- Document generation (correspondence, reports, evaluations)
- Data analysis (trend identification, anomaly detection, summarization)
- Process automation (workflows, notifications, routing)
- Reference lookup (MCOs, regulations, policy interpretation)
- Training development (lesson plans, scenarios, assessments)

**Deliverable:** Each participant produces a one-page frontier map for their functional area. These maps are published on the EDD site and shared with GySgt Swickard. Assign an owner responsible for updating as capabilities change.

**Key teaching point:** This map is the most valuable artifact any of your students will produce. A good frontier map prevents the 19-percentage-point performance drop that the BCG-Harvard study documented for untrained users working outside the frontier.

### Module 2: Complex Build — Multi-Component System (60 min)

Build a system that requires switching between centaur and cyborg modes. Example: a tutoring management system with a data backend (centaur — careful schema design, human-verified), an input interface (cyborg — iterative), and an automated reporting dashboard (centaur — accuracy-critical).

**Before each phase, participants must explicitly identify:**
1. Which mode they're using
2. Why this phase demands that mode
3. What verification looks like at the phase boundary

**Instructor note:** The goal is not a polished product. The goal is conscious, deliberate mode-switching. If a student builds the whole thing in one mode, they haven't learned the skill.

### Module 3: Verification Protocols and QA (45 min)

**Build a QA checklist for your domain.**

For any AI-generated output in your functional area, what must be checked?
1. **Source verification:** Are cited references real? (AI fabricates references.)
2. **Data accuracy:** Do numbers, dates, and identifiers match source material?
3. **Logic check:** Does the reasoning hold? Are conclusions supported by the evidence?
4. **Format compliance:** Does it match required templates, styles, and standards?
5. **Domain review:** Does it reflect how your organization actually operates?

**Exercise:** Take an AI-generated document from your domain. Run it through the checklist. Time yourself. Compare QA time vs. creation-from-scratch time. In most cases, QA of AI output is 30-50% of creation time — that's your time savings.

**Key teaching point:** The GDPval study found that human experts averaged 7 hours per task. AI-assisted workflows with human review were 1.4x faster and 1.6x cheaper. The review step is not overhead — it's where the value is created. An unchecked AI output is a liability, not an asset.

### Module 4: Group Debugging — Real Problems (45 min)

Participants bring actual tools with actual problems.
- 5–10 minutes per problem
- Group collaborates on diagnosis and fix
- Document every failure case — these feed the frontier map

**Structure:**
1. Presenter: "Here's what it should do. Here's what it actually does."
2. Group: Ask clarifying questions
3. Together: Attempt the fix using AI assistance
4. Document: What was the root cause? Was this a frontier issue or a context issue?

### Module 5: Teaching Others — The 201 Multiplier (30 min)

**The problem:** Individual AI capability doesn't scale. Organizational AI capability does. How do you turn what you know into what your section knows?

**The permission gap:** Mollick's research shows that many workers are already using AI but hiding it because they're worried about organizational reaction. Workers are "just not telling management about it." They're worried about job outcomes and don't want to give saved time back. This creates a shadow AI culture where best practices aren't shared and mistakes aren't caught.

**Exercise: The 60-Second Teach**

Each participant picks one of the six 201 skills and teaches it to a partner in 60 seconds, using an example from their own work. Then switch.

This exercise surfaces who can explain the framework clearly — those people are your future instructors.

**Discussion: The Apprentice Problem**

Cite the research: entry-level job postings dropped 35% from 2023-2025. AI is automating the routine tasks that junior workers traditionally learned on. If we're not careful, we'll produce Marines who can operate AI-assisted workflows but can't function when the AI is unavailable.

**The protocol:** When junior Marines use AI in your section:
- Require them to review AI output and explain *why* it's correct or incorrect — not just accept it
- Periodically have them do key tasks without AI to build baseline competency
- Use AI output as a teaching tool: "Here's what AI generated. What would you change and why?"
- Rotate who does QA review — this is where judgment develops

**Deliverable: Workflow Playbook**

Each participant writes a one-page playbook for one AI-integrated workflow in their section:

| Field | Content |
|-------|---------|
| Task | What recurring task does this playbook cover? |
| Frequency | How often is this task performed? |
| Mode | Centaur or Cyborg? |
| Steps | Step-by-step with Human/AI labels for each step |
| Verification Checklist | What must be checked before the output is used? |
| Known Frontier Issues | Where has AI failed on this task? What to watch for? |
| Time Savings Estimate | How long without AI vs. with AI? |
| Junior Development Note | How does this workflow preserve skill-building opportunities? |

These playbooks are published on the EDD site. They become the institutional knowledge base for AI-integrated workflows.

---

## Course 5: Supervisor Orientation

**Recommended | 30 Minutes | Leadership**

**Purpose:** The highest-leverage 30 minutes in the entire program. One supervisor creating a permission culture enables an entire section. One supervisor creating a fear culture kills adoption across their command.

**Prerequisite:** None. Can be taken independently. Recommended before or alongside any other course.

### Core Message

**"Your people are waiting for you to say yes."**

Mollick's research is explicit: workers are already using AI and hiding it. They're worried about organizational reaction. The permission gap — not the technology — is the largest barrier to AI adoption. The DoW's January 2026 AI Strategy pushes "rapid adoption," but rapid adoption requires supervisor support at the lowest levels.

### Agenda

| Time | Module | Focus |
|------|--------|-------|
| 0:00–0:05 | Why This Matters Now | The 80% problem, DoW AI Strategy, rapid adoption mandate |
| 0:05–0:12 | What EDD Is | Five courses, six 201 skills, research foundation (headline level) |
| 0:12–0:20 | Your Role: Creating Permission | What "yes" looks like, what kills adoption, guard rails |
| 0:20–0:27 | Evaluating Proposals and Output | Four questions to ask, what quality looks like |
| 0:27–0:30 | The Apprentice Problem | Preserving junior development while gaining efficiency |

### Module 1: Why This Matters Now (5 min)

- Microsoft found that most workers abandon AI tools within weeks without organizational support
- The UK government study showed 25 minutes/day savings with proper support
- The DoW's January 2026 AI Strategy declares this the year of Military AI Dominance
- The Army created a dedicated AI/ML officer career field (49B)
- The Marine Corps is running generative AI workshops at Quantico
- Your command directed AI as a SITREP item. This isn't optional anymore.

### Module 2: What EDD Is (7 min)

**The four-layer framework:**
1. SOP — Governance: How tools are proposed, reviewed, approved, and maintained
2. Training — Education: Five courses from universal fluency to advanced building
3. QA — Quality: Peer review, security assessment, and user verification
4. Community — Sustainability: Shared frontier maps, workflow playbooks, cross-unit mentoring

**The six 201 skills** (headline level only): Context Assembly, Quality Judgment, Task Decomposition, Iterative Refinement, Workflow Integration, Frontier Recognition. These are management skills, not technical skills. Your Marines already have the foundation.

### Module 3: Your Role — Creating Permission (8 min)

**What "yes" looks like:**
- "Try it. Show me what you build."
- "I don't need to understand how it works. I need to understand what it does."
- "If it saves time and meets quality standards, we should do it."
- Protected time for learning (even 30 minutes per week)
- Public recognition when someone builds something useful

**What kills adoption:**
- "I need to approve every AI interaction."
- "Don't use AI for anything official."
- "We'll wait until there's a formal policy."
- Punishing experimentation failures
- Treating AI use as suspicious or lazy

**Guard rails (not roadblocks):**
- All tools go through the EDD SOP process before deployment
- All AI-generated content for official use must be reviewed by a qualified human
- Sensitive/classified information is never entered into unauthorized AI systems
- Failed experiments are shared as learning, not punished as waste

**The default answer should be "yes, with appropriate review."**

### Module 4: Evaluating Proposals and Output (7 min)

**When a Marine comes to you with an AI-built tool or AI-assisted output, ask four questions:**
1. **Does it work?** Can you demonstrate it doing what it claims to do?
2. **Is the output accurate?** Are facts, references, and numbers verifiable?
3. **Does it follow the SOP?** Has it gone through the proper review process?
4. **Does it save time?** What's the before/after comparison?

**You don't need to evaluate *how* the AI produced it.** That's like asking how the engine works before you'll drive the car. Focus on the output, not the mechanism.

**What quality looks like:** The same standards you'd apply to any work product. If a Marine handed you this document without mentioning AI, would you sign it? That's the standard.

### Module 5: The Apprentice Problem (3 min)

**The long-term risk:** If AI handles all the routine tasks that junior Marines used to learn on, those Marines never develop the judgment to operate without AI. Research shows entry-level employment in AI-exposed fields dropped 13% since 2022. We cannot let this happen in our formations.

**Your role:**
- Require juniors to *review* AI output, not just accept it
- Periodically require key tasks to be done without AI assistance
- Use AI output as a teaching tool: "The AI generated this. What would you change?"
- Ensure developmental assignments aren't all delegated to AI
- Remember: the goal is AI-augmented Marines, not AI-dependent ones

**Closing:** The EDD program provides the structure. Your people have the motivation. The DoW has directed the priority. The only thing standing between your section and measurable productivity gains is your permission. Give it.

---

## Appendices

### Appendix A: Instructor Notes

**AI Fluency Fundamentals:**
- This is the most important course in the program. It determines whether people become sustained AI users or part of the 80% who quit.
- The Red Pen Review exercise is non-negotiable. Skip nothing else, but do not skip this.
- Keep the energy conversational. This is not a PowerPoint death march. Use real examples from your own experience.
- When students say "I don't have time for this," respond with the UK government data: 25 minutes saved per day. The training pays for itself in the first week.
- If teaching online: the Red Pen Review exercise can be done with distributed documents and a timed review period. The debrief must be synchronous.

**Builder Orientation:**
- Verify AI Fluency Fundamentals completion before proceeding.
- The live build should be messy. When something breaks, celebrate it. "This is what debugging looks like. This is normal."
- The decomposition exercise in Module 4 is the make-or-break moment. Students who decompose well build well. Spend the time.

**Platform Training:**
- Failure sharing during break is not optional. It's one of the most valuable 15 minutes in the whole program.
- Build #3 (their own problem) is where students transition from following along to independent capability. Give them space.
- The frontier map produced in the final module is a real deliverable. It should be published on the EDD site.

**Advanced Workshop:**
- The frontier maps and workflow playbooks are deliverables, not exercises. They should be published on the EDD site and shared with GySgt Swickard for MCCES-wide distribution.
- The apprentice problem discussion is particularly important for senior NCOs and staff NCOs attending. They set the tone for their sections.
- The 60-Second Teach exercise identifies future instructors. Note who does it well.

**Supervisor Orientation:**
- This is the highest-leverage 30 minutes in the program. One supervisor creating permission culture enables an entire section.
- Respect the time constraint. Supervisors are busy. Don't run over 30 minutes.
- Lead with operational relevance: SITREP requirement, DoW AI Strategy, what other units are doing.
- End with a specific ask: "Within the next week, ask one person in your section what they'd build if they had permission."

### Appendix B: Quick Reference — The Six 201 Skills

| Skill | One-Sentence Definition | Key Question |
|-------|------------------------|--------------|
| Context Assembly | Curate the right background, constraints, and examples before asking AI to work | "What does AI need to know to do this well?" |
| Quality Judgment | Spot reliable vs. unreliable content in the same output | "Would I sign this?" |
| Task Decomposition | Break work into subtasks and decide which to delegate | "If this were a new Marine, how would I assign it?" |
| Iterative Refinement | Treat AI output as a starting point and direct structured revision | "What's wrong with this draft, specifically?" |
| Workflow Integration | Embed AI into recurring processes so it becomes standard operating procedure | "Can anyone in my section follow this playbook?" |
| Frontier Recognition | Know where AI excels and where it fails for your specific work | "Has anyone tried this task with AI? What happened?" |

### Appendix C: Centaur vs. Cyborg Quick Reference

| Dimension | Centaur | Cyborg |
|-----------|---------|--------|
| Pattern | Clear division between human and AI work | Fluid, continuous integration |
| Best For | High-stakes, accountability-critical work | Creative, iterative, exploratory work |
| Verification | At defined phase boundaries | Continuous throughout |
| DoW Examples | Legal reviews, fitreps, admin separations, personnel actions | SOPs, training materials, PowerApps, data analysis |
| Risk If Misapplied | Too slow for iterative work; AI phases are under-utilized | Insufficient verification for high-stakes work; errors slip through |

### Appendix D: Workflow Playbook Template

| Field | Content |
|-------|---------|
| **Task** | [What recurring task does this playbook cover?] |
| **Frequency** | [How often? Daily / Weekly / Monthly / As needed] |
| **Mode** | [Centaur / Cyborg] |
| **Step 1** | [Description] — **Human / AI** |
| **Step 2** | [Description] — **Human / AI** |
| **Step 3** | [Description] — **Human / AI** |
| **Step 4** | [Description] — **Human / AI** |
| **Step 5** | [Description] — **Human / AI** |
| **Verification Checklist** | [What must be checked?] |
| **Known Frontier Issues** | [Where has AI failed? What to watch for?] |
| **Time Savings** | [Without AI: ___ min/hrs. With AI: ___ min/hrs.] |
| **Junior Development** | [How does this workflow preserve skill-building?] |

### Appendix E: Research Sources

| Study | Key Finding | How We Use It |
|-------|-------------|---------------|
| Microsoft enterprise deployment data (300K+ employees) | 80% of workers abandon AI tools within ~3 weeks; survivors learn management skills, not prompting skills | Opens every course; frames AI as management skill |
| Dell'Acqua et al. (2023), Harvard/BCG, 758 consultants | Inside frontier: +12% tasks, +25% speed, +40% quality. Outside frontier: -19 pp correctness. Two patterns: centaur and cyborg | Core framework: jagged frontier, centaur/cyborg, frontier mapping |
| Brynjolfsson, Li, & Raymond (2025), Stanford/MIT, 5,172 agents | AI boosts productivity 15% avg; novices gain 34%; top performers gain little; AI disseminates tacit expert knowledge | Explains why quality judgment matters more than prompting; supports skill-leveling argument |
| Mollick (2026), Wharton executive MBA experiment | Management skills predict AI effectiveness; delegation equation; "AI work is delegation work" | Management framing throughout; delegation equation in Course 1 Module 3 |
| OpenAI GDPval (2025), 1,320 tasks across 44 occupations | Frontier models approach expert parity on ~48% of tasks; 100x faster/cheaper on inference; 1.4x faster with human review; human oversight remains essential | Validates focus on judgment over prompting; used in Modules on Quality Judgment |
| UK Government Digital Services (2025), 20,000 employees | 25 min/day savings; 80%+ wouldn't give up AI; 9/12 departments continued licenses | Evidence for Supervisor Orientation; proves government bureaucracies benefit from AI with proper training |
| Stanford Digital Economy Lab / multiple (2023-2025) | Entry-level postings down 35%; employment ages 22-25 down 13% in AI-exposed fields; apprenticeship pipeline at risk | Apprentice problem discussion in Advanced Workshop and Supervisor Orientation |
| DoW AI Strategy (January 2026) | "2026 will be the year we raise the bar for Military AI Dominance"; directs rapid adoption across all services | Operational relevance throughout; cited in Supervisor Orientation |
| Army 49B AI/ML Officer career field (October 2025) | Dedicated uniformed AI expertise pathway | Context for why military is investing in AI human capital |

---

## Document History

| Version | Date | Changes |
|---------|------|---------|
| v1.0 | 2024 | Initial training outlines |
| v2.0 | 2024 | Added Power Platform focus |
| v3.0 | Jan 2026 | Restructured around EDD SOP, added QA framework |
| v4.0 | Feb 2026 | Restructured around 201-level skills, jagged frontier, centaur/cyborg patterns, quality judgment exercises, frontier mapping |
| v5.0 | Feb 2026 | Added AI Fluency Fundamentals as universal 5th course; expanded research foundation with GDPval, Mollick delegation equation, Brynjolfsson skill-leveling, UK Government study; added delegation equation module; strengthened apprentice problem protocols; added junior development field to workflow playbooks; full research source appendix |

---

**Classification:** UNCLASSIFIED // Distribution Unlimited

**Proponent:** MCCES AI Working Group / Expert-Driven Development Program

**Published:** https://jeranaias.github.io/ExpertDrivenDevelopment/
